{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab7277f5-f4c2-42ac-9a72-fcbe500ba131",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Python for Social Science"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cafdf52-bbe9-41cb-b4cb-b55f17332d07",
   "metadata": {},
   "source": [
    "<img src=\"../figures/PySocs_banner.png\" width=\"50%\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fa0a2c-4aab-41cb-98b0-768049967870",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Regression and Classification\n",
    "\n",
    "In social science research, understanding the relationships between variables‚Äîparticularly to the extent of predicting outcomes‚Äîis a central goal. Two widely used statistical approaches for achieving these objectives are regression and classification methods.\n",
    "\n",
    "<img src=\"../figures/regression_classification.png\" alt=\"Regression vs. Classification\" width=\"50%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738dbfe4-7709-4ada-8018-03cfb09f0c12",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Regression\n",
    "\n",
    "Regression is a statistical method used to examine the relationship between a continuous outcome variable (dependent variable) and one or more predictor variables (independent variables). Multiple regression, which uses two or more predictors, enables researchers to predict the outcome variable, control for additional factors, and quantify the unique contribution of each predictor.\n",
    "\n",
    "**Example in social science**:\n",
    "For example, a researcher might investigate how educational attainment, income, and work experience affect political participation. In this case, political participation‚Äîsuch as the amount of political contributions made‚Äîserves as the dependent variable, while education, income, and work experience are the predictor variables.\n",
    "\n",
    "Key features:\n",
    "\n",
    "- Estimates the magnitude and direction of relationships.\n",
    "- Provides predictions for the dependent variable based on the predictors.\n",
    "- Can test hypotheses about which factors are significant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b76286-d997-4575-988a-ae264bf52ac5",
   "metadata": {},
   "source": [
    "### Classification\n",
    "\n",
    "Classification methods are used when the outcome variable is categorical rather than continuous. Instead of predicting a numeric value, classification predicts membership in a category.\n",
    "\n",
    "**Example in social science**:\n",
    "Predicting whether someone votes (yes/no) based on age, gender, income, and education. Here, the dependent variable is binary (voter/non-voter), so classification methods like logistic regression.\n",
    "\n",
    "Key features:\n",
    "\n",
    "- Assigns observations to categories based on predictor variables.\n",
    "- Provides probabilities of category membership (e.g., probability of voting).\n",
    "- Useful for policy analysis, survey analysis, and behavioral prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3df4b8-54b4-4a8c-97a3-e1cc3200d118",
   "metadata": {},
   "source": [
    "### Why These Methods Matter\n",
    "\n",
    "Both multiple regression and classification allow social scientists to make sense of complex social data. They help answer questions like:\n",
    "\n",
    "- What factors predict social behavior?\n",
    "- How do multiple social and economic factors interact to shape outcomes?\n",
    "- Can we anticipate future trends based on current data?\n",
    "\n",
    "In Python, these methods are often implemented using tools like `statsmodels` and `scikit-learn` , which allow for flexible modeling, visualization, and evaluation of results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0999a81e-304f-4719-a624-01aad4ec0a72",
   "metadata": {},
   "source": [
    "## Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58add58-9c82-45a1-ae91-1bb68e4b7c40",
   "metadata": {
    "tags": []
   },
   "source": [
    "### statsmodels vs. scikit-learn\n",
    "\n",
    "Both `statsmodels` and `scikit-learn` can fit regression models, but they serve slightly different purposes and workflows.\n",
    "\n",
    "1Ô∏è‚É£ **Statsmodels**\n",
    "\n",
    "- Focus: Statistical inference and model interpretation.\n",
    "- Key function: `OLS(y, X).fit()`\n",
    "- Strengths:\n",
    "    - Provides detailed regression output: coefficients, standard errors, t-tests, p-values, confidence intervals, R¬≤, F-statistics.\n",
    "    - Great for hypothesis testing and understanding the effect of each predictor.\n",
    "\n",
    "- Workflow:\n",
    "    - Prepare predictors and add constant (sm.add_constant(X)).\n",
    "    - Fit the model: \n",
    "        - `model = OLS(y, X).fit()`.\n",
    "    - Inspect: `model.summary()`.\n",
    "\n",
    "- ‚úÖ Best when your goal is statistical analysis and interpretability.\n",
    "\n",
    "\n",
    "2Ô∏è‚É£ **Scikit-Learn**\n",
    "\n",
    "- Focus: Machine learning and prediction.\n",
    "- Key function: `LinearRegression().fit(X, y)`\n",
    "- Strengths:\n",
    "    - Designed for prediction pipelines (a structured sequence of steps that can combine preprocessing, cross-validation, and more).\n",
    "    - Integrates easily with other ML models and metrics like RMSE, R¬≤, and cross-validation.\n",
    "\n",
    "- Workflow:\n",
    "    - Prepare predictors (no need to manually add a constant; intercept handled automatically).\n",
    "    - Fit the model:\n",
    "        - `model = LinearRegression()`\n",
    "        - `model.fit(X, y)`\n",
    "        \n",
    "    - Predict and evaluate: `model.predict(X_test)`\n",
    "\n",
    "‚úÖ Best when your goal is prediction or machine learning pipelines, not detailed statistical inference.\n",
    "\n",
    "**Summary**\n",
    "\n",
    "| Feature   | Statsmodels                            | Scikit-Learn                       |\n",
    "| --------- | -------------------------------------- | ---------------------------------- |\n",
    "| Goal      | Statistical inference                  | Prediction & ML pipelines          |\n",
    "| Intercept | Must add manually                      | Handled automatically              |\n",
    "| Output    | Detailed summary (p-values, SE, R¬≤, F) | Coefficients, R¬≤, RMSE (numerical) |\n",
    "| Use case  | Hypothesis testing, research papers    | Model prediction, ML workflows     |\n",
    "\n",
    "üí° **Rule of thumb**:\n",
    "\n",
    "- Use `statsmodels` if you care about interpreting coefficients and statistical significance.\n",
    "- Use `scikit-learn` if you care about building predictive models or combining with ML workflows.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb23216-4278-4ec5-b207-a2415e7e3ea9",
   "metadata": {},
   "source": [
    "### Predicting Median House Values\n",
    "\n",
    "The California Housing Dataset is a classic dataset in regression analysis. It contains information about housing characteristics in California districts (sampling unitws) collected from the 1990 Census. The main goal is often to predict the median house value (`MedHouseVal`) based on various features of each district.\n",
    "\n",
    "$$\n",
    "\\text{MedHouseVal} = \\beta_0 + \\beta_1 \\text{MedInc} + \\beta_2 \\text{HouseAge} + \\cdots + \\varepsilon\n",
    "$$\n",
    "\n",
    "In the dataset, each observation corresponds to a block group (a small geographic area) with features such as:\n",
    "\n",
    "**Dataset Overview**\n",
    "\n",
    "- Target variable:\n",
    "    - `MedHouseVal` ‚Äì median house value in units of $100,000s\n",
    "\n",
    "- Predictor variables:\n",
    "\n",
    "| Feature       | Description                                |\n",
    "|---------------|--------------------------------------------|\n",
    "| MedInc        | Median income in the district              |\n",
    "| HouseAge      | Median age of houses                        |\n",
    "| AveRooms      | Average number of rooms per household      |\n",
    "| AveBedrms     | Average number of bedrooms per household   |\n",
    "| Population    | Population in the district                 |\n",
    "| AveOccup      | Average household occupancy                |\n",
    "| Latitude / Longitude | Geographic coordinates                |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f7e848-3f61-4340-bdcf-7d746b4e8643",
   "metadata": {},
   "source": [
    "### Import modeuls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766b4069-35db-435a-8d5a-80dfeee11f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "\n",
    "import sklearn\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3711013c-5225-4aaf-b988-a755b0adac45",
   "metadata": {},
   "source": [
    "#### Multiple linear regression in statsmodels\n",
    "\n",
    "**Statsmodels** is oriented toward traditional statistical modeling: hypothesis tests, estimation, inference. It provides linear models (OLS, GLS, WLS), generalized linear models (GLMs), discrete choice models (logit, probit), etc.\n",
    "\n",
    "**Pros**\n",
    "\n",
    "| Strength                                  | Details                                                                                                                                                                                             |\n",
    "| ----------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **Rich inference output**                 | P-values, confidence intervals, test statistics, goodness‚Äêof‚Äêfit measures, diagnostics. Good when you need to do hypothesis testing or interpret parameters. |\n",
    "| **Formula / R-style interface**           | Ability to define models via formulas, handling categorical variables, etc., which helps in statistical modeling.                                            |\n",
    "| **Econometric / time-series / GLM tools** | Generous support for ARIMA, VAR, things like discrete choice models, robust regression, etc.                                                                          |\n",
    "| **Diagnostics / assumptions**             | Tools to check residuals, heteroscedasticity, autocorrelation, etc., which are often necessary for statistical work.                                                                                |\n",
    "\n",
    "**Cons**\n",
    "\n",
    "| Weakness                                                | Details                                                                                                                                                                                |\n",
    "| ------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **Less ‚Äúplug-and-play‚Äù for ML pipelines**               | Fewer built-in tools for cross-validation, hyperparameter tuning, ensembling, etc. So if the goal is prediction accuracy with many model choices, scikit-learn tends to give you more. |\n",
    "| **Performance / scalability in large ML settings**      | May be slower on large datasets or when using more computationally intensive methods compared to optimized ML libraries.                                                               |\n",
    "| **Less unified API across all models**                  | Because of its focus on statistical correctness and varied model types, the interfaces and behavior across models can be less uniform, which can be a small hassle.                    |\n",
    "| **Less recent focus on ‚Äúblack-box‚Äù predictive methods** | If you want gradient boosting, random forests, etc., statsmodels is not the go-to (though there are bridging / wrapping options).                                                      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7b3e23-9cea-47d2-991c-011b45af8e03",
   "metadata": {},
   "source": [
    "### üìä California Housing Data\n",
    "\n",
    "- `fetch_california_housing(as_frame=True)` returns a `pandas.DataFrame` with features and the target (`MedHouseVal`).\n",
    "- If you run this the first time your environment needs internet to download the dataset; later it is cached by sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bca0b74-362d-4ecc-88f1-0f7379523435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the California housing dataset (this fetches data from sklearn)\n",
    "data = fetch_california_housing(as_frame=True)   # requires internet first time\n",
    "df = data.frame.copy()  # includes features and target\n",
    "df_housing = df.copy()  # make a copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6459225a-3de6-4a7e-b144-4df414d41952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect\n",
    "print(\"Columns:\", list(df.columns))\n",
    "print(\"\\n\", df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf67acb-ddb5-4633-905a-4d39c60a2af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_housing.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcad0936-18e3-4f19-b750-3f4ad780e7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_housing.corr().round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419d8919-6458-44e2-8aa4-ed4d7eebf952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlation matrix\n",
    "corr_matrix = df_housing.corr()\n",
    "\n",
    "# Set up the figure\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Draw the heatmap\n",
    "sns.heatmap(corr_matrix, \n",
    "            annot=True,       # Show correlation values\n",
    "            fmt=\".2f\",        # Format numbers\n",
    "            cmap=\"RdBu\",  # Color map\n",
    "            square=True,      # Square cells\n",
    "            cbar_kws={\"shrink\": 0.8})  # Colorbar size\n",
    "\n",
    "plt.title(\"Correlation Matrix of Housing Data\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb07b69e-e826-4e6c-855c-dc07d3ee711c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize distributions\n",
    "\n",
    "df.hist(bins=50, figsize=(12, 8));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e5a8d2-8dc8-4130-aca6-e9ee1bfdf6d1",
   "metadata": {},
   "source": [
    "### Transforming variables\n",
    "\n",
    "Why skewness matters\n",
    "\n",
    "- Linear regression assumptions:\n",
    "    - Linearity: predictors have a roughly linear relationship with the outcome.\n",
    "    - Homoscedasticity: constant variance of residuals.\n",
    "    - Normality of residuals (for inference, less critical for prediction).\n",
    "\n",
    "Skewed predictors can cause:\n",
    "\n",
    "- Nonlinear-looking relationships.\n",
    "- Inflated influence of extreme values (outliers).\n",
    "- Instability in interaction or polynomial terms.\n",
    "\n",
    "Two predictor variables, `Population` and `AveOccup`, have highly skewed distributions. We'll begin by visualizing them to determine if a transformation is necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8114d6c2-4432-430c-8d71-30958caed7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_log_histogram(series, log_func=np.log, bins=30):\n",
    "    \"\"\"\n",
    "    Plots histograms of the original variable and its log-transformed version.\n",
    "    \n",
    "    Parameters:\n",
    "    - series: pandas Series, the variable to transform\n",
    "    - log_func: function to apply for log transformation (default: natural log)\n",
    "    - bins: number of histogram bins (default: 30)\n",
    "    \"\"\"\n",
    "    # Handle zeros or negative values\n",
    "    if (series <= 0).any():\n",
    "        print(\"Warning: Non-positive values detected. Using log1p transformation.\")\n",
    "        log_series = np.log1p(series)\n",
    "        log_label = \"log1p(\" + series.name + \")\" # log(1 + x)\n",
    "    else:\n",
    "        log_series = log_func(series)\n",
    "        log_label = \"log(\" + series.name + \")\"\n",
    "    \n",
    "    # Plot\n",
    "    # Create 2x2 subplot grid\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 6), gridspec_kw={'height_ratios': [3, 1]})\n",
    "    \n",
    "    # --- Histograms ---\n",
    "    axes[0, 0].hist(series, bins=bins, color='skyblue', edgecolor='black')\n",
    "    axes[0, 0].set_title(f'Original {series.name}')\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "    \n",
    "    axes[0, 1].hist(log_series, bins=bins, color='salmon', edgecolor='black')\n",
    "    axes[0, 1].set_title(f'Log-transformed {series.name}')\n",
    "    axes[0, 1].set_ylabel('Frequency')\n",
    "    \n",
    "    # --- Boxplots ---\n",
    "    axes[1, 0].boxplot(series.dropna(), vert=False, patch_artist=True, boxprops=dict(facecolor='skyblue'))\n",
    "    axes[1, 0].set_xlabel(series.name)\n",
    "    \n",
    "    axes[1, 1].boxplot(log_series.dropna(), vert=False, patch_artist=True, boxprops=dict(facecolor='salmon'))\n",
    "    axes[1, 1].set_xlabel(log_label)\n",
    "    \n",
    "    plt.tight_layout();\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08ccc88-0d20-40ff-b69f-251176441ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_log_histogram(df.Population, bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2b0e6a-584b-4a85-8830-fe243559ae6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_log_histogram(df.AveOccup, bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57a5c76-c53b-4ec8-8b1e-163882717468",
   "metadata": {},
   "source": [
    "Based on the visualization of the variables, it seems appropriate to log-transform them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934509dd-0ae2-4416-8789-058ad3ff5fcf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_housing['Population'] = np.log(df_housing['Population'])\n",
    "df_housing['AveOccup'] = np.log(df_housing['AveOccup'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f9101d-7ad3-4930-b94f-85335feb0393",
   "metadata": {
    "tags": []
   },
   "source": [
    "### üè° Pile-up of expensive houses in MedHouseVal\n",
    "\n",
    "The variable `MedHouseVal` (median house value for each census block group) is censored at \\\\$500,000 ‚Äî this was the maximum allowed value in the original 1990 California census data. Many neighborhoods had actual median values above \\\\$500,000, but they were all recorded as exactly `5.000` (in \\\\$100,000).\n",
    "\n",
    "Removing them introduces selection bias (systematically excludes high-value neighborhoods). The regression slope estimates will be biased downward, since the upper tail is truncated. Treating them as if they are uncensored will also introduce a bias in the estimates.\n",
    "\n",
    "There have been both arguments for and against removing observations where `MedHouseValue > 5` in analyses using the California Housing dataset. \n",
    "\n",
    "| Argument Type               | Keep Capped Values          | Remove Capped Values              |\n",
    "| --------------------------- | --------------------------- | --------------------------------- |\n",
    "| **Data completeness**       | ‚úîÔ∏è Keeps full dataset       | ‚ùå Removes censored portion        |\n",
    "| **Model bias**              | ‚ùå Introduces downward bias  | ‚úîÔ∏è Reduces censoring bias         |\n",
    "| **Representation**          | ‚úîÔ∏è Keeps high-value areas   | ‚ùå Misses high-end housing markets |\n",
    "| **Statistical assumptions** | ‚ùå Violated by truncation    | ‚úîÔ∏è More valid for OLS/log models  |\n",
    "\n",
    "For simplicity, we will proceed without removing the censored cases; however, it's important to be aware that this may violate OLS assumptions and introduce bias. A more rigorous approach would use a **Tobit model** or censored regression to explicitly address truncation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559f39e3-8a9e-4144-90b0-33821c23ed58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log-transform MedHouseVal\n",
    "df_housing[\"MedHouseVal\"] = np.log(df_housing[\"MedHouseVal\"])\n",
    "\n",
    "# Create side-by-side histograms\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Original distribution\n",
    "sns.histplot(df[\"MedHouseVal\"], kde=True, ax=axes[0], color=\"skyblue\", bins=40)\n",
    "axes[0].set_title(\"Distribution of Median House Value (Raw)\")\n",
    "axes[0].set_xlabel(\"MedHouseVal ($100,000s)\")\n",
    "axes[0].set_ylabel(\"Frequency\")\n",
    "\n",
    "# Log-transformed distribution\n",
    "sns.histplot(df_housing[\"MedHouseVal\"], kde=True, ax=axes[1], color=\"salmon\", bins=40)\n",
    "axes[1].set_title(\"Distribution of log(MedHouseVal)\")\n",
    "axes[1].set_xlabel(\"log(MedHouseVal)\")\n",
    "axes[1].set_ylabel(\"Frequency\")\n",
    "\n",
    "plt.suptitle(\"Effect of Log-Transformation on Median House Value\", fontsize=14)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46de5b6d-9abc-472b-8467-c0201f21d748",
   "metadata": {},
   "source": [
    "**Left plot (Raw)**: You‚Äôll see a right-skewed distribution ‚Äî many homes are inexpensive, with a long tail of high values.\n",
    "\n",
    "**Right plot (Log)**: The log-transformed values appear more symmetric and bell-shaped, making them more suitable for linear regression assumptions (normality & homoscedasticity)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda0ed63-732f-4248-9a8f-9471a62b844f",
   "metadata": {},
   "source": [
    "**Censored MedHousingVal**\n",
    "\n",
    "Because all census blocks with median home value ‚â• $500,000 were simply recorded as 5.0, the upper tail of the distribution is artificially compressed.\n",
    "\n",
    "That causes several issues:\n",
    "\n",
    "- Bias in regression coefficients ‚Äî OLS assumes continuous, uncensored outcomes. The cap makes the relationship nonlinear and biased at the top end.\n",
    "- Non-normal residuals ‚Äî predictions for high-income areas will systematically undershoot.\n",
    "- Influence on fit metrics ‚Äî metrics like RMSE or $R^2$ can be misleadingly low, since the model can‚Äôt predict beyond the ceiling.\n",
    "- Misleading visualization ‚Äî histograms or scatterplots of MedHouseValue vs predictors show a ‚Äúflat ceiling‚Äù at 5.0.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd16716-450d-46e6-88e3-d8d10d35c8af",
   "metadata": {},
   "source": [
    "#### Truncation / Winsorization\n",
    "\n",
    "Truncating extreme values (e.g., top 1‚Äì2%) can reduce the influence of outliers. The technical term is *winsorization*, which is a statistical method for handling extreme values, or outliers, by replacing them with less extreme values within the dataset. Instead of removing outliers, it \"caps\" them at a specific percentile.\n",
    "\n",
    "Two candidates for winsorization are `AveRooms` and `AveBedrms`. The following will clip the top 1% for the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeeb0dc3-2631-4ad2-a6fa-452cef6cb198",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats.mstats import winsorize\n",
    "\n",
    "# Winsorize: cap lowest 5% and highest 5%\n",
    "df_housing['AveRooms'] = winsorize(df_housing['AveRooms'], limits=[0.00, 0.01])\n",
    "df_housing['AveBedrms'] = winsorize(df_housing['AveBedrms'], limits=[0.00, 0.01])\n",
    "\n",
    "df_housing.hist(bins=50, figsize=(12, 8));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e23ad6b-1604-41ca-bb30-542e7ab0a627",
   "metadata": {},
   "source": [
    "#### Visualizing Median House Value in California\n",
    "\n",
    "The following visualization clearly illustrates the geographic distribution of the sampling districts in California, along with their corresponding median house values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88921027-de1e-4d21-af35-db1cd51d8aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Scatter Plot: Raw Prices ---\n",
    "# Create scatterplot with continuous hue\n",
    "sns.scatterplot(\n",
    "    data=df,\n",
    "    x=\"Longitude\", y=\"Latitude\",\n",
    "    hue=\"MedHouseVal\",\n",
    "    palette=\"viridis\",\n",
    "    alpha=0.6,\n",
    "    edgecolor=None,\n",
    "    s=20\n",
    ")\n",
    "\n",
    "# Add title and axis labels\n",
    "plt.title(\"Geographic Distribution of Median House Value in California\")\n",
    "plt.xlabel(\"Longitude (¬∞)\")\n",
    "plt.ylabel(\"Latitude (¬∞)\")\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa85ac0-f748-43c2-90ec-da89d377c3c5",
   "metadata": {},
   "source": [
    "**Scatter Plot**\n",
    "\n",
    "- Each dot is a neighborhood.\n",
    "- The color (from purple ‚Üí yellow) represents median house value (low to high).\n",
    "- You‚Äôll notice:\n",
    "    - High values (bright yellow) cluster along the coastline and around San Francisco and Los Angeles.\n",
    "    - Lower values (dark blue/purple) appear inland and in the Central Valley.\n",
    "\n",
    "This spatial pattern immediately suggests that location (longitude and latitude) drives much of the price variation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3ed407-8e88-4b31-a6e9-79edc0787863",
   "metadata": {},
   "source": [
    "---\n",
    "**Training/Test split**\n",
    "\n",
    "Splitting data into **training** and **test** subsets helps evaluate how well a regression model generalizes to unseen data.\n",
    "\n",
    "- The training set is used to estimate model parameters (fit the model).\n",
    "- The test set is held out and used only for performance evaluation.\n",
    "\n",
    "This separation prevents overfitting ‚Äî when a model captures noise or random fluctuations specific to the training data rather than the true underlying relationship.\n",
    "\n",
    "By comparing model performance on both sets (e.g., using RMSE or $R^2$), we obtain an unbiased estimate of predictive accuracy and ensure the model will perform reliably on new observations.\n",
    "\n",
    "We will perform a 80/20 split so evaluation is out-of-sample. `random_state=42` keeps results reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2052d9-98a6-49a5-b90c-9193eeaf5aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "train_df, test_df = train_test_split(df_housing, test_size=0.2, random_state=42)\n",
    "print(f\"Train size: {len(train_df)}, Test size: {len(test_df)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74e72f0-5059-40c4-92cc-6124efb5d11d",
   "metadata": {},
   "source": [
    "---\n",
    "**Fit OLS with statsmodels**\n",
    "\n",
    "- We use the formula API: `MedHouseVal ~ MedInc + HouseAge + ....`\n",
    "- `model.summary()` prints coefficient estimates, standard errors, t-statistics, p-values, R¬≤, adjusted R¬≤, AIC/BIC, and other diagnostics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46643a4d-0091-42ee-8666-73646846846a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define formula and fit OLS using statsmodels formula API\n",
    "feature_cols = [c for c in df_housing.columns if c not in [\"MedHouseVal\"]]\n",
    "formula = \"MedHouseVal ~ \" + \" + \".join(feature_cols)\n",
    "print(\"Formula:\", formula)\n",
    "\n",
    "model = smf.ols(formula=formula, data=train_df).fit()\n",
    "print(\"\\n\", model.summary())    # main summary output: coefficients, R-squared, p-values, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc17a17-f03e-447a-99b8-c16ed09c3a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coefficients table with confidence intervals\n",
    "coef_df = pd.DataFrame({\n",
    "    \"coef\": model.params.round(4),\n",
    "    \"std_err\": model.bse.round(4),\n",
    "    \"t\": model.tvalues.round(4),\n",
    "    #\"pvalue\": model.pvalues,\n",
    "    \"pvalue\": [f\"{p:8.4f}\" for p in model.pvalues],\n",
    "    #\"pvalue:\": model.pvalues.round(4),\n",
    "    \"ci_lower\": model.conf_int().iloc[:,0].round(4),\n",
    "    \"ci_upper\": model.conf_int().iloc[:,1].round(4)\n",
    "})\n",
    "print(\"\\nCoefficients and 95% CIs:\")\n",
    "print(coef_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fa3494-f7f1-4ac7-8c33-1bcd0086a0e9",
   "metadata": {},
   "source": [
    "**Interpret coefficients & CIs**\n",
    "\n",
    "- Each coef tells the (estimated) change in `MedHouseVal` per unit change in that predictor, holding others constant.\n",
    "- Look at pvalue for statistical significance (conventionally <0.05) and 95% CI to see plausible ranges.\n",
    "\n",
    "**What the log transformation means**\n",
    "\n",
    "Because the dependent variable (`MedHouseVal`) is in log form, the coefficients can be interpreted as approximate percentage changes in the original median house value:\n",
    "\n",
    "That is one-unit change in $X_j$ is associated with an approximate $100 \\times \\beta_j$ percent change in $Y$.\n",
    "\n",
    "As a concrete example, let's consider $\\beta_{\\text{Longitude}}=0.2755$, which can be interpreted as follows:\n",
    "\n",
    "All else being equal (*ceteris paribus*), for a one-unit increase in longitude (1 degree $\\approx$ 55-70 miles):\n",
    "\n",
    "$$\n",
    "    \\Delta \\text{log}(\\text{MedHouseVal}) = -0.2755\n",
    "$$\n",
    "\n",
    "Exponentiate to get the multiplicative effect on the original dollar scale:\n",
    "\n",
    "$$\n",
    "    e^{-0.2755} - 1 \\approx -0.2408\n",
    "$$\n",
    "\n",
    "So:\n",
    "\n",
    "Holding other variables constant, moving one degree of longitude estward is associated with about 24.1% decrease in median house value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a61064a-3294-423a-9673-3fd2e089cfca",
   "metadata": {},
   "source": [
    "---\n",
    "### Practice Exercise 1\n",
    "\n",
    "Interpret the following regression coefficient:\n",
    "\n",
    "$$\\beta_{\\text{MedInc}} = 0.1791$$\n",
    "\n",
    "YOUR RESPONSE HERE:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3471f7ec-e7f0-413a-9aa9-cb8293d22059",
   "metadata": {},
   "source": [
    "---\n",
    "#### Multicollinearity Check\n",
    "\n",
    "**VIF (variance inflation factor)**\n",
    "\n",
    "The VIF (Variance Inflation Factor) measures how much a given predictor is linearly related to the others ‚Äî essentially, how much multicollinearity exists.\n",
    "\n",
    "- VIF quantifies multicollinearity. Rules of thumb:\n",
    "    - VIF ‚âà 1 ‚Üí little/no collinearity\n",
    "    - VIF > 5 ‚Üí concerning\n",
    "    - VIF > 10 ‚Üí serious multicollinearity\n",
    "- If you see high VIFs, consider removing or combining correlated predictors, or use PCA/regularized regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0176d993-83e9-4668-ac16-6286fbb2f0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VIF calculation (multicollinearity check)\n",
    "X = sm.add_constant(train_df[feature_cols])  # design matrix with intercept\n",
    "vif = pd.DataFrame({\n",
    "    \"feature\": X.columns,\n",
    "    \"VIF\": [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "})\n",
    "print(\"\\nVIFs (include constant):\")\n",
    "print(vif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b4d300-114c-4173-9c68-704d7d8a7879",
   "metadata": {},
   "source": [
    "Because of the geographical shape of California, i.e., spanning NW to SE, the `Lattitude` and `Longitude` of its districts are strongly correlated, causing multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875f46f2-2d68-41af-8d76-113a082b9bed",
   "metadata": {},
   "source": [
    "---\n",
    "#### Homoscedasticity Check\n",
    "\n",
    "**Diagnostic plots**\n",
    "\n",
    "- Residuals vs fitted: should look like a horizontal band around zero. Patterns (curvature, funnel shape) indicate model misspecification or heteroscedasticity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a922582-4ccf-4bc1-a6c0-a01b3573c14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract fitted values and residuals\n",
    "fitted = model.fittedvalues\n",
    "resid = model.resid\n",
    "\n",
    "# Diagnostics plots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# --- Scatter plot ---\n",
    "axes[0].scatter(fitted, resid, alpha=0.2)\n",
    "axes[0].axhline(0, linestyle=\"--\", color=\"red\")\n",
    "axes[0].set_xlabel(\"Fitted values\")\n",
    "axes[0].set_ylabel(\"Residuals\")\n",
    "axes[0].set_title(\"Residuals vs Fitted (scatter)\")\n",
    "\n",
    "# --- Colored density / contour plot ---\n",
    "sns.kdeplot(\n",
    "    x=fitted, y=resid,\n",
    "    fill=False, cmap=\"coolwarm\",  # colored density\n",
    "    thresh=0, levels=100,        # smooth contours\n",
    "    ax=axes[1]\n",
    ")\n",
    "axes[1].axhline(0, linestyle=\"--\", color=\"black\")\n",
    "axes[1].set_xlabel(\"Fitted values\")\n",
    "axes[1].set_ylabel(\"Residuals\")\n",
    "axes[1].set_title(\"Residuals vs Fitted (density contour)\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaac336f-91f9-4821-b871-0b6a8c19c8a9",
   "metadata": {},
   "source": [
    "- Q-Q plot: checks residual normality; deviations in tails indicate heavy tails or outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebb0a6c-e83f-463a-9d25-b558bd6a70b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "# Assume resid is your residuals array\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# --- Q-Q Plot ---\n",
    "sm.qqplot(resid, line=\"45\", fit=True, ax=axes[0])\n",
    "axes[0].set_title(\"Q-Q plot of residuals\")\n",
    "\n",
    "# --- Density Plot ---\n",
    "sns.kdeplot(resid, fill=True, ax=axes[1], color=\"skyblue\")\n",
    "\n",
    "# Overlay normal distribution\n",
    "mu, sigma = np.mean(resid), np.std(resid)\n",
    "x = np.linspace(resid.min(), resid.max(), 100)\n",
    "axes[1].plot(x, norm.pdf(x, mu, sigma), color='red', linestyle='--', label=\"Normal fit\")\n",
    "\n",
    "axes[1].set_title(\"Density of residuals with normal overlay\")\n",
    "axes[1].set_xlabel(\"Residual\")\n",
    "axes[1].set_ylabel(\"Density\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7963c8c-1cc3-46be-9dd9-3361222d3c55",
   "metadata": {},
   "source": [
    "**Q-Q Plot Interpretation**\n",
    "\n",
    "The Q-Q plot reveals leptokurtic residuals (heavy tails). The residuals near 0 are approximately normal. So the model is fitting the majority of the data reasonably well. Approximately 95% of the residuals (the middle portion) are well-fitted by the model, following a normal-like pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14521de1-4810-4a5b-aa7e-5caeeea974d8",
   "metadata": {},
   "source": [
    "**Breusch-Pagan Test**\n",
    "\n",
    "The Breusch-Pagan test is a statistical test used in regression analysis to detect heteroscedasticity, which occurs when the variance of the residuals is not constant across levels of the independent variables:\n",
    "\n",
    "$$\n",
    "    \\text{Var}(\\epsilon_i)=\\sigma^2, \\hspace{0.3cm} \\forall_i\n",
    "$$\n",
    "\n",
    "- Formal test for heteroscedasticity. Low p-value (e.g., <0.05) suggests heteroscedastic errors.\n",
    "- If heteroscedasticity is present, consider robust (HC) standard errors or use weighted least squares.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89c2b50-9425-4da0-9cc5-fce0c8b26c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Breusch‚ÄìPagan test for heteroscedasticity\n",
    "bp_test = het_breuschpagan(resid, model.model.exog)\n",
    "bp_labels = [\"LM Statistic\", \"LM p-value\", \"F Statistic\", \"F p-value\"]\n",
    "\n",
    "print(\"\\nBreusch‚ÄìPagan Test for Heteroscedasticity\")\n",
    "print(\"-\" * 42)\n",
    "for name, value in zip(bp_labels, bp_test):\n",
    "    print(f\"{name:15s}: {value:>8.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c2d09b-52de-44c5-9d34-8824b9ab6b0d",
   "metadata": {},
   "source": [
    "**Breusch-Pagan Test Interpretation**\n",
    "\n",
    "The Breusch-Pagan test ($p<0.001$) suggests that the variance of the residuals depends on the independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e35feaf-34c3-48f3-aed4-5ac8926e9bd9",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "#### Rationale for Evaluating a Regression Model on New Data\n",
    "\n",
    "When building a regression model, the ultimate goal is to make accurate predictions or understand relationships that generalize beyond the dataset used for model training. Evaluating the model on new, unseen (test) data serves several important purposes:\n",
    "\n",
    "- Assess Generalization Performance\n",
    "- Detect Overfitting or Underfitting\n",
    "- Compare Models or Methods\n",
    "- Quantify Predictive Uncertainty\n",
    "\n",
    "Evaluating a regression model on new data ensures that the model‚Äôs predictive capabilities are realistic, reliable, and generalizable, rather than being an artifact of the training dataset. It is a critical step in model validation and deployment.\n",
    "\n",
    "**Out-of-sample evaluation**\n",
    "\n",
    "When evaluating a regression model, RMSE (Root Mean Squared Error) and R¬≤ (coefficient of determination) are the most common metrics.\n",
    "\n",
    "- Compute RMSE and $R^2$ on the test set. These numbers tell you predictive performance on unseen data.\n",
    "- $R^2$ close to training $R^2$ suggests no overfitting; a much lower test $R^2$ suggests overfitting or that the model poorly generalizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c5c5e1-45ce-43de-8a13-869acf0f6483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Train performance ---\n",
    "y_train = train_df[\"MedHouseVal\"]\n",
    "y_train_pred = model.fittedvalues\n",
    "\n",
    "rmse_train = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "r2_train = r2_score(y_train, y_train_pred)\n",
    "\n",
    "# --- Test performance ---\n",
    "y_test = test_df[\"MedHouseVal\"]\n",
    "y_test_pred = model.predict(test_df)\n",
    "\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "r2_test = r2_score(y_test, y_test_pred)\n",
    "\n",
    "# --- Create comparison table ---\n",
    "metrics = pd.DataFrame({\n",
    "    \"RMSE\": [rmse_train, rmse_test],\n",
    "    \"R2\": [r2_train, r2_test]\n",
    "}, index=[\"Train\", \"Test\"])\n",
    "\n",
    "print(metrics.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959cda68-2743-46f2-9169-ec850519e7d9",
   "metadata": {},
   "source": [
    "**RMSE and $R^2$ Interpretation**\n",
    "\n",
    "Both RMSE and $R^2$ from the unseen data are only slightly worse than those from the train data, suggesting that the model is generalizable and not overfitted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df390c8-4d57-426e-9a23-fff4c58cc068",
   "metadata": {},
   "source": [
    "**Actual vs predicted**\n",
    "\n",
    "Comparing the observed and fitted values within each dataset can provide some insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06036dc5-840a-49ae-a31e-323a0069506c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine common axis limits\n",
    "min_val = min(y_train.min(), y_test.min())\n",
    "max_val = max(y_train.max(), y_test.max())\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6), sharex=True, sharey=True)\n",
    "\n",
    "# Training data scatterplot\n",
    "sns.scatterplot(x=y_train, y=y_train_pred, ax=axes[0], color='blue', alpha=0.6)\n",
    "axes[0].plot([min_val, max_val], [min_val, max_val], 'k--', lw=2)\n",
    "axes[0].set_title(\"Train Data\")\n",
    "axes[0].set_xlabel(\"Observed\")\n",
    "axes[0].set_ylabel(\"Predicted / Fitted\")\n",
    "\n",
    "# Add RMSE and R¬≤ text\n",
    "train_rmse = metrics.loc['Train', 'RMSE']\n",
    "train_r2 = metrics.loc['Train', 'R2']\n",
    "axes[0].text(\n",
    "    0.05, 0.95,\n",
    "    f\"RMSE = {train_rmse:.3f}\\nR¬≤ = {train_r2:.3f}\",\n",
    "    transform=axes[0].transAxes,\n",
    "    verticalalignment='top',\n",
    "    bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='white', alpha=0.7)\n",
    ")\n",
    "\n",
    "# Test data scatterplot\n",
    "sns.scatterplot(x=y_test, y=y_test_pred, ax=axes[1], color='red', alpha=0.6)\n",
    "axes[1].plot([min_val, max_val], [min_val, max_val], 'k--', lw=2)\n",
    "axes[1].set_title(\"Test Data\")\n",
    "axes[1].set_xlabel(\"Observed\")\n",
    "axes[1].set_ylabel(\"Predicted / Fitted\")\n",
    "\n",
    "# Add RMSE and R¬≤ text\n",
    "test_rmse = metrics.loc['Test', 'RMSE']\n",
    "test_r2 = metrics.loc['Test', 'R2']\n",
    "axes[1].text(\n",
    "    0.05, 0.95,\n",
    "    f\"RMSE = {test_rmse:.3f}\\nR¬≤ = {test_r2:.3f}\",\n",
    "    transform=axes[1].transAxes,\n",
    "    verticalalignment='top',\n",
    "    bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='white', alpha=0.7)\n",
    ")\n",
    "\n",
    "\n",
    "plt.suptitle(\"Observed vs Fitted Values: Train vs Test\", fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42245fc9-fb90-4742-8303-cb9fbe2d117f",
   "metadata": {},
   "source": [
    "#### Should longitude and latitude be included?\n",
    "\n",
    "Including Longitude and Latitude makes sense because location is a major determinant of housing prices ‚Äî coastal areas (e.g., near San Francisco or Los Angeles) are much more expensive than inland regions.\n",
    "\n",
    "So, longitude and latitude capture spatial variation that other features cannot.\n",
    "They help control for geographic effects in prices, acting as proxies for location.\n",
    "\n",
    "While useful, longitude and latitude are not linear predictors in a geographic sense.\n",
    "Prices do not change in a straight-line pattern as you move east or north ‚Äî spatial relationships are nonlinear and interactive.\n",
    "\n",
    "**A simple linear model**:\n",
    "\n",
    "$$\n",
    "\\ln(\\text{MedHouseVal}) = \\beta_0 + \\beta_1 \\text{Longitude} + \\beta_2 \\text{Lattitude} + \\cdots + \\varepsilon\n",
    "$$\n",
    "\n",
    "assumes a flat plane over California ‚Äî which is unrealistic.\n",
    "\n",
    "**A more realistic model** might include polynomial or interaction terms:\n",
    "\n",
    "$$\n",
    "\\ln(\\text{MedHouseVal}) = \\beta_0 + \\beta_1 \\text{Longitude} + \\beta_2 \\text{Lattitude} + \\beta_3 \\text{(Longitude x Lattitude)} + \\cdots + \\varepsilon\n",
    "$$\n",
    "\n",
    "or even use spatial models, splines, or tree-based methods that can capture nonlinear spatial effects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f574a1b1-484b-41e9-882a-a9e379355ee2",
   "metadata": {},
   "source": [
    "**Interpreting the coefficients**:\n",
    "\n",
    "If you include them linearly, interpretation goes like this:\n",
    "\n",
    "| Variable           | Interpretation (log model)                                                                                                                                                                                                                       |\n",
    "| ------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n",
    "| **Longitude (Œ≤‚ÇÅ)** | The expected **% change in median house value** for a one-degree move east (longitude increases), holding other factors constant. <br> In California, longitude increases eastward, so Œ≤‚ÇÅ is typically **negative** (prices drop moving inland). |\n",
    "| **Latitude (Œ≤‚ÇÇ)**  | The expected **% change in median house value** for a one-degree move north (latitude increases), holding other factors constant. <br> Œ≤‚ÇÇ is often **positive**, reflecting higher prices in northern coastal cities (e.g., SF area).            |\n",
    "\n",
    "Because a degree of longitude/latitude spans large distances (~100 km), these coefficients capture broad spatial gradients ‚Äî not local effects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2def3f-1f72-4861-bb21-9e5723fff130",
   "metadata": {},
   "source": [
    "**Summary**\n",
    "\n",
    "| Question                               | Answer                                                                                               |\n",
    "| :------------------------------------- | :--------------------------------------------------------------------------------------------------- |\n",
    "| Should longitude/latitude be included? | Yes ‚Äî they capture spatial price patterns.                                                           |\n",
    "| Is a linear relationship adequate?     | Only as a rough approximation; nonlinear or interaction terms are often better.                      |\n",
    "| How to interpret coefficients?         | Approx. % change in median house value per degree change east/north, holding other factors constant. |\n",
    "| Caveat                                 | They‚Äôre proxies for spatial effects, not causal predictors. Use cautiously.                          |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76896dca-5aa7-40f7-aa77-291ad0716171",
   "metadata": {},
   "source": [
    "#### Should longitude x latitude be included?\n",
    "\n",
    "Interaction term: `Longitude √ó Latitude`\n",
    "\n",
    "- This term captures a *diagonal* effect:\n",
    "- How the effect of `Longitude` depends on `Latitude`, and vice versa.\n",
    "\n",
    "Conceptually, does moving east‚Äìwest have a different effect in the north vs. the south?\n",
    "\n",
    "‚ÄúNot significant‚Äù means there‚Äôs no strong evidence that such a diagonal relationship exists in the data.\n",
    "\n",
    "If \"Not significant\", the increase or decrease in house value doesn‚Äôt change depending on the combination of longitude and latitude ‚Äî the interaction doesn‚Äôt add meaningful information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ada372-1885-463e-8fbd-ab3dfb5a388a",
   "metadata": {},
   "source": [
    "#### Should Longitude¬≤ and Latitude¬≤ be included?\n",
    "\n",
    "These terms capture curvature along each axis individually:\n",
    "\n",
    "- Longitude¬≤ ‚Üí how the effect of longitude accelerates (convex or concave)\n",
    "- Latitude¬≤ ‚Üí how the effect of latitude accelerates\n",
    "\n",
    "Significant quadratic terms imply an acceleration or deceleration effect along the longitude or latitude axes.\n",
    "\n",
    "Conceptually:\n",
    "\n",
    "- House values might rise more quickly in certain ranges of longitude (west vs. east) or latitude (north vs. south) ‚Äî even without a diagonal interaction.\n",
    "   \n",
    "- This creates high-value or low-value clusters along the north-south or east-west axes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bcfebb-3c1f-4a60-9b68-27c769503f44",
   "metadata": {},
   "source": [
    "#### Models to be compared\n",
    "\n",
    "We will consider the following three models and compare them using different criteria.\n",
    "\n",
    "\\begin{array}{|l|l|}\n",
    "\\hline\n",
    "\\textbf{Model} & \\textbf{Equation} \\\\\n",
    "\\hline\n",
    "\\text{Model 1: Baseline} & \n",
    "\\begin{aligned}\n",
    "\\ln(\\text{MedHouseVal}) = & \\; \\beta_0 + \\beta_1 \\text{Longitude} + \\beta_2 \\text{Latitude} \\\\\n",
    "& + \\beta_3 \\text{MedInc} + \\beta_4 \\text{HouseAge} \\\\\n",
    "& + \\beta_5 \\text{AveRooms} + \\beta_6 \\text{AveOccup} \\\\\n",
    "& + \\varepsilon\n",
    "\\end{aligned} \\\\\n",
    "\\hline\n",
    "\\text{Model 2: Baseline + Interaction} & \n",
    "\\begin{aligned}\n",
    "\\ln(\\text{MedHouseVal}) = & \\; \\beta_0 + \\beta_1 \\text{Longitude} + \\beta_2 \\text{Latitude} \\\\\n",
    "& + \\beta_3 (\\text{Longitude} \\times \\text{Latitude}) \\\\\n",
    "& + \\beta_4 \\text{MedInc} + \\beta_5 \\text{HouseAge} \\\\\n",
    "& + \\beta_6 \\text{AveRooms} + \\beta_7 \\text{AveOccup} \\\\\n",
    "& + \\varepsilon\n",
    "\\end{aligned} \\\\\n",
    "\\hline\n",
    "\\text{Model 3: Model 2 + Quadratic Coordinates} & \n",
    "\\begin{aligned}\n",
    "\\ln(\\text{MedHouseVal}) = & \\; \\beta_0 + \\beta_1 \\text{Longitude} + \\beta_2 \\text{Latitude} \\\\\n",
    "& + \\beta_3 (\\text{Longitude} \\times \\text{Latitude}) \\\\\n",
    "& + \\beta_4 \\text{Longitude}^2 + \\beta_5 \\text{Latitude}^2 \\\\\n",
    "& + \\beta_6 \\text{MedInc} + \\beta_7 \\text{HouseAge} \\\\\n",
    "& + \\beta_8 \\text{AveRooms} + \\beta_9 \\text{AveOccup} \\\\\n",
    "& + \\varepsilon\n",
    "\\end{aligned} \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ad3437-31aa-4ffb-bd09-1d2d517ae4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define formulas for the three models\n",
    "\n",
    "# Model 1: Baseline\n",
    "formula1 = \"MedHouseVal ~ Longitude + Latitude + MedInc + HouseAge + AveRooms + AveOccup\"\n",
    "\n",
    "# Model 2: Baseline + Interaction\n",
    "formula2 = \"MedHouseVal ~ Longitude * Latitude + MedInc + HouseAge + AveRooms + AveOccup\"\n",
    "# Note: Longitude * Latitude automatically includes Longitude, Latitude, and their interaction\n",
    "\n",
    "# Model 3: Model 2 + Quadratic coordinates\n",
    "formula3 = \"MedHouseVal ~ Longitude * Latitude + I(Longitude**2) + I(Latitude**2) + MedInc + HouseAge + AveRooms + AveOccup\"\n",
    "\n",
    "# Fit the models\n",
    "model1 = smf.ols(formula=formula1, data=df_housing).fit()\n",
    "model2 = smf.ols(formula=formula2, data=df_housing).fit()\n",
    "model3 = smf.ols(formula=formula3, data=df_housing).fit()\n",
    "\n",
    "# Model summary\n",
    "\n",
    "print(\"\\n---MODEL 2---\\n\", model2.summary())\n",
    "print(\"\\n---MODEL 3---\\n\", model3.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7056f0-7fe0-41fc-81c1-b8398b63b708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize fit indices\n",
    "fit_summary = pd.DataFrame({\n",
    "    \"Model\": [\"Baseline\", \"Baseline + Interaction\", \"Interaction + Quadratic Coords\"],\n",
    "    \"R^2\": [model1.rsquared, model2.rsquared, model3.rsquared],\n",
    "    \"Adj. R^2\": [model1.rsquared_adj, model2.rsquared_adj, model3.rsquared_adj],\n",
    "    \"AIC\": [model1.aic, model2.aic, model3.aic],\n",
    "    \"BIC\": [model1.bic, model2.bic, model3.bic]\n",
    "})\n",
    "\n",
    "print(\"=== Model Fit Summary ===\")\n",
    "print(fit_summary)\n",
    "\n",
    "# Nested F-tests\n",
    "f_test_1vs2 = model2.compare_f_test(model1)\n",
    "f_test_2vs3 = model3.compare_f_test(model2)\n",
    "\n",
    "print(\"\\n=== Nested F-tests ===\")\n",
    "print(f\"Model 1 vs Model 2: F = {f_test_1vs2[0]:7.3f}, p = {f_test_1vs2[1]:.4f}, df_diff = {f_test_1vs2[2]}\")\n",
    "print(f\"Model 2 vs Model 3: F = {f_test_2vs3[0]:7.3f}, p = {f_test_2vs3[1]:.4f}, df_diff = {f_test_2vs3[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26def97-b2a8-49cd-b5f8-6b802373cb7f",
   "metadata": {},
   "source": [
    "---\n",
    "**ANOVA Test**\n",
    "\n",
    "We can use the `stats.anova_lm()` to compare the model and test if the added variables improve the model fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40665b1-c32a-4ef3-aebf-bd8872bbeadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "anova_results = sm.stats.anova_lm(model1, model2, model3)\n",
    "print(anova_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ebb91b-3205-4a6f-b2c2-0c9188ce1f2c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "---\n",
    "#### Interpretation of model comparisons\n",
    "\n",
    "From the model comparisons, the following conclusions seem appropriate:\n",
    "\n",
    "- Significant interaction, even more significant quadratics ‚Üí there is a strong curvature effect along each axis individually, and some combined diagonal effect.\n",
    "\n",
    "You can think of it as:\n",
    "\n",
    "- The ‚Äúsurface‚Äù of predicted house values is bowl-shaped, ridge-shaped, or dome-shaped along longitude and latitude, and it tilted diagonally.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa910459-96fc-46af-8cea-a2df0173f963",
   "metadata": {},
   "source": [
    "---\n",
    "### Multiple linear regression in Scikit-Learn\n",
    "\n",
    "Scikit-learn (sklearn) provides an easy-to-use interface for building regression models. The `LinearRegression()` class from scikit-learn does not support a formula argument (like \"y ~ x1 + x2 + ...\").\n",
    "\n",
    "We will use the California Housing dataset again and compare the same three models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126bb484-aab7-4d96-8767-4f5dcbdc756e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features\n",
    "X_base = df_housing[[\"Longitude\", \"Latitude\", \"MedInc\", \"HouseAge\", \"AveRooms\", \"AveOccup\"]]\n",
    "y = df_housing[\"MedHouseVal\"]\n",
    "\n",
    "# Model 1: Baseline\n",
    "md1 = LinearRegression()\n",
    "md1.fit(X_base, y)\n",
    "y_pred1 = md1.predict(X_base)\n",
    "\n",
    "# Model 2: Baseline + Interaction (pairwise only for longitude √ó latitude)\n",
    "poly2 = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "X_interact = poly2.fit_transform(df_housing[[\"Longitude\", \"Latitude\"]])\n",
    "# Concatenate other predictors\n",
    "X2 = np.hstack([X_interact, df_housing[[\"MedInc\", \"HouseAge\", \"AveRooms\", \"AveOccup\"]].values])\n",
    "\n",
    "md2 = LinearRegression()\n",
    "md2.fit(X2, y)\n",
    "y_pred2 = md2.predict(X2)\n",
    "\n",
    "# Model 3: Baseline + Interaction + Quadratic (degree 2 full poly for lon/lat)\n",
    "poly3 = PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)\n",
    "X_poly = poly3.fit_transform(df_housing[[\"Longitude\", \"Latitude\"]])\n",
    "# Concatenate other predictors\n",
    "X3 = np.hstack([X_poly, df_housing[[\"MedInc\", \"HouseAge\", \"AveRooms\", \"AveOccup\"]].values])\n",
    "\n",
    "md3 = LinearRegression()\n",
    "md3.fit(X3, y)\n",
    "y_pred3 = md3.predict(X3)\n",
    "\n",
    "# Evaluate models\n",
    "def evaluate_model(y_true, y_pred, model_name):\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    print(f\"{model_name}: R^2 = {r2:.4f}, RMSE = {rmse:.4f}\")\n",
    "\n",
    "evaluate_model(y, y_pred1, \"Baseline               \")\n",
    "evaluate_model(y, y_pred2, \"Baseline + Interaction \")\n",
    "evaluate_model(y, y_pred3, \"Interaction + Quadratic\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04901229-7558-4973-a703-a06942e07019",
   "metadata": {},
   "source": [
    "---\n",
    "### Practice Exercise 2\n",
    "\n",
    "You are studying the determinants of wages using data from the 1985 Current Population Survey (CPS).\n",
    "The dataset contains 534 observations on U.S. workers, including demographics, education, and job characteristics.\n",
    "\n",
    "| Variable     | Description                                |\n",
    "| ------------ | ------------------------------------------ |\n",
    "| `wage`       | hourly wage (in USD)                       |\n",
    "| `education`  | years of education                         |\n",
    "| `experience` | years of potential labor market experience |\n",
    "| `age`        | age in years                               |\n",
    "| `gender`     | male or female                             |\n",
    "| `occupation` | occupation category                        |\n",
    "| `sector`     | public or private sector                   |\n",
    "| `union`      | union membership (yes/no)                  |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9892f91-a0b7-4b4f-b4cb-80598ec601fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df_cps1985 = sm.datasets.get_rdataset(\"CPS1985\", \"AER\").data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e977312-a5b6-47cd-85ff-7b9d858d56f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check correlations\n",
    "df_cps1985.select_dtypes(include='number').corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724043cf-93c7-4e78-bdb9-8c66a51d3d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the DV\n",
    "plot_log_histogram(df_cps1985.wage, bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237fec85-0b91-44c0-a62f-5321ea48c94e",
   "metadata": {},
   "source": [
    "---\n",
    "**Step 0** Transform wage\n",
    "\n",
    "Create `log_wage` and add to `df_cps1985`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cb78b5-f931-453e-8f08-a217d2879705",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ab813d-4ebb-4ad4-837b-8a86eb18a73c",
   "metadata": {},
   "source": [
    "---\n",
    "**Step 1** Fit Model 1:\n",
    "\n",
    "$\\ln(wage_i) = \\beta_0 + \\beta_1 education_i + \\beta_2 experience_i + \\epsilon_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4d87b8-be9f-49e0-afbf-8eb06c4a4f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655233bc-a6ab-43b0-933b-bfa7643d0fb8",
   "metadata": {},
   "source": [
    "---\n",
    "**Step 2** Fit Model 2:\n",
    "\n",
    "$\\ln(wage_i) = \\beta_0 + \\beta_1 education_i + \\beta_2 experience_i + \\beta_3 age_i + \\beta_4 gender_i + \\epsilon_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d4a1ce-17a8-4bf8-8cdf-e632e753c342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5591e5-668b-4fff-9290-2c94271276e9",
   "metadata": {},
   "source": [
    "---\n",
    "**Step 3** Fit Model 3\n",
    "\n",
    "$\\ln(wage_i) = \\beta_0 + \\beta_1 education_i + \\beta_2 experience_i + \\beta_3 age_i + \\beta_4 gender_i + \\beta_5 union_i + \\sum_j \\beta_{6j} occupation_{ij} + \\epsilon_i$\n",
    "\n",
    "Make sure `occupation` is treated as a categorical variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dda7e03-880b-45fc-a895-94d7dae2eb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e1bca3-fa58-4438-93a0-65c5e9ace0b3",
   "metadata": {},
   "source": [
    "---\n",
    "**Step 4**: Compare Nested Models\n",
    "\n",
    "Use ANOVA to test if the added variables improve model fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22aa22d7-58e4-4cca-9439-eb0920114f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "anova_results = sm.stats.anova_lm(model1, model2, model3)\n",
    "print(anova_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ab5988-e6f7-41b4-b360-5edacd2afd68",
   "metadata": {},
   "source": [
    "---\n",
    "**Step 5**: Model Diagnostics (on Full Model)\n",
    "\n",
    "Plot Residuals vs Fitted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1da82a0-2aa6-4cc2-a5fa-6cc3a981e576",
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = model3.resid\n",
    "fitted = model3.fittedvalues\n",
    "\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4c56de-ba4a-4223-bb6d-6862e2663724",
   "metadata": {},
   "source": [
    "Plot a Q-Q plot on Residuals and a density function side-by-side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40c7d7c-33fe-437e-8f86-03ae0a1f93ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# --- Q-Q Plot ---\n",
    "sm.qqplot(residuals, line=\"45\", fit=True, ax=axes[0])\n",
    "axes[0].set_title(\"Q-Q plot of residuals\")\n",
    "\n",
    "# --- Density Plot ---\n",
    "sns.kdeplot(residuals, fill=True, ax=axes[1], color=\"skyblue\")\n",
    "\n",
    "# Overlay normal distribution\n",
    "mu, sigma = np.mean(residuals), np.std(residuals)\n",
    "x = np.linspace(residuals.min(), residuals.max(), 100)\n",
    "axes[1].plot(x, norm.pdf(x, mu, sigma), color='red', linestyle='--', label=\"Normal fit\")\n",
    "\n",
    "axes[1].set_title(\"Density of residuals with normal overlay\")\n",
    "axes[1].set_xlabel(\"Residual\")\n",
    "axes[1].set_ylabel(\"Density\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af77972f-b659-4324-ac5e-c9f0ed922dab",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Classification\n",
    "\n",
    "$$\n",
    "\\begin{array}{c|cc|c}\n",
    " & \\text{Predicted 0} & \\text{Predicted 1} & \\text{Totals} \\\\\n",
    "\\hline\n",
    "\\text{Actual 0} & \\text{TN} & \\text{FP} & \\text{TN + FP} \\\\\n",
    "\\text{Actual 1} & \\text{FN} & \\text{TP} & \\text{FN + TP} \\\\\n",
    "\\hline\n",
    "\\text{Totals} & \\text{TN + FN} & \\text{FP + TP} & \\text{N}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "In social and behavioral sciences, researchers often want to understand how a set of predictors influences a binary outcome ‚Äî for example:\n",
    "\n",
    "- Does a student get admitted to graduate school (yes/no)?\n",
    "- Does someone vote or not vote?\n",
    "- Does an individual smoke or not smoke?\n",
    "- Does a patient develop a coronary heart disease in 10 years or not?\n",
    "\n",
    "These are classification problems, not regression problems in the ordinary least squares (OLS) sense, because the outcome is categorical.\n",
    "\n",
    "When the outcome Y is binary (0 or 1), we can use **logistic regression** ‚Äî a member of the generalized linear model (GLM) family ‚Äî to model the probability that Y=1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83783a90-ab5c-4237-9bf4-1d52285c4994",
   "metadata": {},
   "source": [
    "### Binary Classification with Logistic Regression \n",
    "\n",
    "Instead of modeling Y directly, logistic regression models the probability $p = P(Y=1)$ through the logit link:\n",
    "\n",
    "$$\n",
    "\\log\\left(\\frac{p}{1 - p}\\right) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_k X_k\n",
    "$$\n",
    "\n",
    "The inverse (logistic) function gives predicted probability:\n",
    "\n",
    "$$\n",
    "p = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 X_1 + \\dots + \\beta_k X_k)}}\n",
    "$$\n",
    "\n",
    "Interpretation: coefficients $\\beta_i$ are in **log-odds** units. Exponentiating yields **odds ratios**: $ \\exp(\\beta_i) $.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efa2b2f-0eed-4716-a4ec-5e5cbbe944eb",
   "metadata": {},
   "source": [
    "### üìä Load the Admissions dataset\n",
    "\n",
    "The UCLA admissions dataset contains 400 applicants to a graduate program with their GRE, GPA, and the prestige of their undergraduate institution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3949abfb-17ad-4f91-8d25-bc3337d099a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "\n",
    "# UCLA Admissions dataset (public domain)\n",
    "#url = \"https://stats.idre.ucla.edu/stat/data/binary.csv\"\n",
    "#admit = pd.read_csv(url)\n",
    "admit = pd.read_csv(\"../data/admit.csv\")\n",
    "\n",
    "# Rename columns for clarity\n",
    "admit.columns = [\"admit\", \"gre\", \"gpa\", \"rank\"]\n",
    "admit[\"rank\"] = admit[\"rank\"].astype(\"category\")\n",
    "\n",
    "# Count missing values per column\n",
    "missing_counts = admit.isnull().sum()\n",
    "print(\"No. Missing Values: \\n\")\n",
    "print(missing_counts, \"\\n\")\n",
    "\n",
    "admit.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528c9e3c-3be4-4296-9a83-1228892eebd8",
   "metadata": {},
   "source": [
    "### Inspect and summarize the data\n",
    "\n",
    "- `admit` is binary (0 = not admitted, 1 = admitted)\n",
    "- `gre` and `gpa` are continuous\n",
    "- `rank` is ordinal categorical (1 = highest prestige, 4 = lowest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f67418b-099b-4b44-8b32-1390ece2f4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "admit.info()\n",
    "print(\"\\n\", admit.admit.value_counts(normalize=False), \"\\n\")\n",
    "admit.describe(include='all').round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36290797-e200-4d4b-81a1-8a30a4d368d2",
   "metadata": {},
   "source": [
    "### Exploratory Data Visualization\n",
    "\n",
    "Exploratory data visualization (EDV) is a core step in data analysis, because it helps you understand your data before modeling.\n",
    "\n",
    "Visualization lets you see the shape and distribution of your data at a glance.\n",
    "You can identify:\n",
    "\n",
    "- The range and central tendency of variables\n",
    "- Skewness, outliers, and missing values\n",
    "- Whether variables are categorical, continuous, or ordinal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98370682-19c2-472f-b7ce-3f2b2ae97643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Marginal and joint distributions\n",
    "g = sns.PairGrid(admit)\n",
    "g.map_diag(sns.histplot)          # diagonal: histograms of each variable\n",
    "#g.map_offdiag(sns.scatterplot)   # optional: scatterplots everywhere\n",
    "g.map_upper(sns.scatterplot)      # upper triangle: scatterplots\n",
    "g.map_lower(sns.kdeplot)          # lower triangle: KDE plots of pairs\n",
    "\n",
    "g.fig.suptitle(\"Pairwise Relationships in the Admissions Dataset\", fontsize=14, y=1.02);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bb0a6c-0b0a-4ddb-b243-1808825bce23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplots\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.boxplot(data=admit, x='admit', y='gre', hue='admit', palette=['lightcoral', 'skyblue'])\n",
    "plt.title('GRE by Admission')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(data=admit, x='admit', y='gpa', hue='admit', palette=['lightcoral', 'skyblue'])\n",
    "plt.title('GPA by Admission')\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafe8e57-df18-4cd8-8110-83ffabb284e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart\n",
    "admit.groupby('rank', observed=True)['admit'].sum().plot(kind='bar')\n",
    "plt.xticks(rotation=0)\n",
    "plt.title('Admissions by Rank')\n",
    "plt.ylabel('Number Admitted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df69e27c-31f9-409f-8197-6a4cf101f1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute counts of admitted vs not admitted by rank\n",
    "counts = pd.crosstab(admit['rank'], admit['admit'], normalize='index')\n",
    "\n",
    "# Plot stacked bar chart\n",
    "counts.plot(kind='bar', stacked=True, color=['lightcoral', 'skyblue'])\n",
    "\n",
    "plt.title(\"Admissions by Rank (UCLA Data)\")\n",
    "plt.xlabel(\"Rank (Prestige of Undergraduate Institution)\")\n",
    "plt.ylabel(\"Proportion\")\n",
    "plt.legend([\"Not Admitted (0)\", \"Admitted (1)\"], title=\"Admission Status\")\n",
    "plt.xticks(rotation=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eeb44d8-1524-42ee-8d90-e8977a538f52",
   "metadata": {},
   "source": [
    "### Fit logistic regression model with `statsmodels`\n",
    "\n",
    "Model:\n",
    "\n",
    "$$\n",
    "admit \\sim gre + gpa + C(rank)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3bef2b-8ac8-4776-8b11-4e865d742a23",
   "metadata": {},
   "source": [
    "The formula: `'admit ~ gre + gpa + C(rank)'`\n",
    "\n",
    "- `admit` ‚Üí dependent variable (binary outcome: admitted or not).\n",
    "- `~` ‚Üí separates dependent variable from predictors.\n",
    "- `gre + gpa + C(rank)` ‚Üí independent variables:\n",
    "    - `gre` ‚Üí numeric predictor (GRE score).\n",
    "    - `gpa` ‚Üí numeric predictor (GPA).\n",
    "    - `C(rank)` ‚Üí categorical predictor. `C()` tells statsmodels to treat rank as categorical and automatically create dummy/indicator variables.\n",
    "- Without `C(rank)`, rank would be treated as numeric, which would be incorrect if rank is an ordinal factor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163d4c87-240c-4fd4-81e6-8dc2930f62e0",
   "metadata": {},
   "source": [
    "The logistic regression model is:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{logit}(\\Pr(\\text{admit}=1)) \n",
    "&= \\log\\left(\\frac{\\Pr(\\text{admit}=1)}{\\Pr(\\text{admit}=0)}\\right) \\\\\n",
    "&= \\beta_0 + \\beta_1 \\cdot \\text{gre} + \\beta_2 \\cdot \\text{gpa} \\\\\n",
    "&\\quad + \\beta_3 \\cdot I(\\text{rank}=2) + \\beta_4 \\cdot I(\\text{rank}=3) + \\beta_5 \\cdot I(\\text{rank}=4)\n",
    "\\end{align*}$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\beta_0$ is the intercept (baseline log-odds for rank=1).  \n",
    "- $\\beta_1, \\beta_2$ are coefficients for GRE and GPA.  \n",
    "- $\\beta_3, \\beta_4, \\beta_5$ are **effects of rank relative to rank 1**.  \n",
    "- $I(\\text{rank}=k)$ is an indicator function: 1 if rank=k, 0 otherwise.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a1c6dc-e4e9-4c88-b354-616a5f45ed95",
   "metadata": {},
   "source": [
    "### Dummy Variable Encoding for `rank` and Model Equation\n",
    "\n",
    "As `rank` has 4 levels (1 = highest prestige, 4 = lowest), `statsmodels` creates **dummy variables** with the first level as reference:\n",
    "\n",
    "| rank | I(rank=2) | I(rank=3) | I(rank=4) |\n",
    "|------|-----------|-----------|-----------|\n",
    "| 1    | 0         | 0         | 0         |\n",
    "| 2    | 1         | 0         | 0         |\n",
    "| 3    | 0         | 1         | 0         |\n",
    "| 4    | 0         | 0         | 1         |\n",
    "\n",
    "- Reference category: **rank=1** ‚Üí absorbed in the intercept $\\beta_0$  \n",
    "- Each coefficient $\\beta_k$ shows the effect relative to rank=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a6939a-de2d-40ff-8e44-0803951bfec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smf.logit('admit ~ gre + gpa + C(rank)', data=admit)\n",
    "result = model.fit(disp=False) # suppress displaying optimization process\n",
    "print(result.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30117f41-4978-4807-bf85-cca676e9e4c5",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "- **GRE** and **GPA** are positively associated with admission probability.\n",
    "- **Rank** (prestige) has a negative effect; applicants from lower-prestige (rank 4) schools have reduced odds of admission.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32adf153-478a-41b9-80c5-666332c575a9",
   "metadata": {},
   "source": [
    "### Odds Ratios\n",
    "\n",
    "In logistic regression, the model estimates coefficients in log-odds: \n",
    "\n",
    "$$\n",
    "    \\log\\left(\\frac{P(Y=1)}{1-P(Y=1)}\\right) = \\beta_0 + \\beta_1 X_1 + \\dots + \\beta_k X_k\n",
    "$$\n",
    "\n",
    "Exponentiating gives the odds ratio (OR):\n",
    "\n",
    "$$\\text{OR} = e^{\\beta}$$\n",
    "\n",
    "**OR > 1** ‚Üí positive association\n",
    "\n",
    "- Increasing $X_i$ decreases the odds of the outcome $Y=1$.\n",
    "\n",
    "**OR < 1** ‚Üí negative association\n",
    "\n",
    "- Increasing $X_i$ decreases the odds of the outcome.\n",
    "\n",
    "**OR = 1** ‚Üí no effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d51b3b-1a9a-4d84-8ef3-41169fb356a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression parameter estimates\n",
    "params = result.params\n",
    "conf = result.conf_int()\n",
    "odds = pd.DataFrame({\n",
    "    'coef': params,\n",
    "    'OR': np.exp(params),\n",
    "    'CI_lower': np.exp(conf[0]),\n",
    "    'CI_upper': np.exp(conf[1])\n",
    "})\n",
    "odds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c83ac42-a5ec-4c2c-bf1e-cde3c74cd54b",
   "metadata": {},
   "source": [
    "**All else equal...**\n",
    "\n",
    "- **GRE coefficient:** $\\beta_1 = 0.002 \\Rightarrow \\text{OR} = e^{0.002} \\approx 1.002$  \n",
    "  - Each additional point in GRE **increases the odds of admission by 0.2\\%**, holding GPA and rank constant.\n",
    "  - GRE scores range from 200 to 800 with a SD of 100, much larger than that of GPA.\n",
    "  - Rescaling $\\text{GRE}^* = \\frac{GRE}{100}$, a 100-point increase in GRE raises the odds of admission by $\\approx 24.6\\%$.\n",
    "\n",
    "- **GPA coefficient:** $\\beta_2 = 0.8 \\Rightarrow \\text{OR} = e^{0.8} \\approx 2.23$  \n",
    "  - Each additional GPA point **more than doubles the odds** of admission.\n",
    "\n",
    "- **Rank 2 vs Rank 1:** $\\beta_3 = -0.675 \\Rightarrow \\text{OR} = e^{-0.675} \\approx 0.51$  \n",
    "  - Being from a **rank 2 school (vs rank 1)** **halves the odds** of admission.\n",
    "\n",
    "- **Rank 3 vs Rank 1:** $\\beta_3 = -1.340 \\Rightarrow \\text{OR} = e^{-1.340} \\approx 0.26$  \n",
    "  - Being from a **rank 3 school (vs rank 1)** **reduces the odds** of admission by 74%.\n",
    "\n",
    "\n",
    "**Interpretation**\n",
    "- Holding other factors constant, an increase of 1 point in GRE increases the odds of admission by 0.2%. \n",
    "- A one-point increase in GPA increases the odds of admission by 123%. \n",
    "- Compared to applicants from rank 1 schools, applicants from rank 2 schools have about half the odds of being admitted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1826cb96-1c46-4bef-9c37-9e76be91757b",
   "metadata": {},
   "source": [
    "### Practical Tip\n",
    "\n",
    "- **Always exponentiate coefficients** to interpret them in terms of odds.  \n",
    "- Remember: **odds $\\neq$ probability**, but they are related:\n",
    "\n",
    "$$\n",
    "\\text{odds} = \\frac{p}{1-p} \\quad \\Rightarrow \\quad p = \\frac{\\text{odds}}{1 + \\text{odds}}\n",
    "$$\n",
    "\n",
    "- Small odds ratios around 1 correspond to **small probability changes**, whereas large odds ratios correspond to **substantial probability changes**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddc70a7-6a38-4241-a0e0-0b3440c32e90",
   "metadata": {},
   "source": [
    "### Model Evaluation: Predictions and Default Threshold (0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d026ea-a596-412f-9c64-7ac7789d6524",
   "metadata": {},
   "outputs": [],
   "source": [
    "admit['pred_prob'] = result.predict(admit)\n",
    "admit['pred_0_5'] = (admit['pred_prob'] >= 0.5).astype(int)\n",
    "\n",
    "cm = confusion_matrix(admit['admit'], admit['pred_0_5'])\n",
    "\n",
    "print('Confusion matrix (threshold=0.5):')\n",
    "print(cm)\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(admit['admit'], admit['pred_0_5']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f698e5bd-ba1b-41e4-a580-287ed2faa403",
   "metadata": {},
   "source": [
    "**Confusion Matrix**\n",
    "\n",
    "|                     | **Predicted Negative** | **Predicted Positive** |\n",
    "| ------------------: | :--------------------: | :--------------------: |\n",
    "| **Actual Negative** |   True Negative (TN)   |   False Positive (FP)  |\n",
    "| **Actual Positive** |   False Negative (FN)  |   True Positive (TP)   |\n",
    "\n",
    "\n",
    "|                     | **Predicted Negative** | **Predicted Positive** |\n",
    "| ------------------: | :--------------------: | :--------------------: |\n",
    "| **Actual Negative** |   254                  |   19                   |\n",
    "| **Actual Positive** |   97                   |   30                   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3c90ff-4092-4988-8168-8bc386f2ff15",
   "metadata": {},
   "source": [
    "| **Modern ML Term** | **Traditional / Classical Term**                                            | **Definition / Formula**                                                                          | **Interpretation**                                                 |\n",
    "| ------------------ | --------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------ |\n",
    "| **Precision**      | **Positive Predictive Value (PPV)**                                         | $\\text{Precision} = \\frac{TP}{TP + FP}$                                                         | Of all predicted positives, how many are truly positive?           |\n",
    "| **Recall**         | **Sensitivity**, **True Positive Rate (TPR)**                               | $\\text{Recall} = \\frac{TP}{TP + FN}$                                                            | Of all actual positives, how many were correctly identified?       |\n",
    "| **F1-score**       | *(no exact classical analog, but)* **Harmonic mean of PPV and Sensitivity** | $F_1 = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}$ | Balances precision and recall; useful when classes are imbalanced. |\n",
    "| **Support**        | **Sample Size per Class**                                                   | Count of true instances for each class                                                            | Indicates how many observations each class has in the test data.   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4522f562-1c3c-4340-841c-463f0afa7c5b",
   "metadata": {},
   "source": [
    "| Metric                         | Formula                                       |                Value               | Interpretation                                       |\n",
    "| :----------------------------- | :-------------------------------------------- | :--------------------------------: | :--------------------------------------------------- |\n",
    "| **Accuracy**                   | ((TP + TN) / (TP + TN + FP + FN))             |      ((30 + 254) / 400 = 0.71)     | 71 % of all cases were correctly classified          |\n",
    "| **Precision (PPV)**            | (TP / (TP + FP))                              |       (30 / (30 + 19) ‚âà 0.61)      | Of all predicted positives, 61 % were truly positive |\n",
    "| **Recall (Sensitivity)**       | (TP / (TP + FN))                              |       (30 / (30 + 97) ‚âà 0.24)      | Only 24 % of true positives were detected            |\n",
    "| **Specificity (TNR)**          | (TN / (TN + FP))                              |      (254 / (254 + 19) ‚âà 0.93)     | 93 % of true negatives were correctly identified     |\n",
    "| **F1-score**                   | $2¬∑\\frac{Precision¬∑Recall}{Precision+Recall}$ | (2¬∑(0.61¬∑0.24)/(0.61+0.24) ‚âà 0.35) | Balanced measure of precision & recall               |\n",
    "| **Prevalence (Actual 1 Rate)** | ((TP + FN) / N)                               |         (127 / 400 ‚âà 0.32)         | 32 % of cases are positive in reality                |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb53354f-a4ae-4257-88f9-a50b3340c199",
   "metadata": {},
   "source": [
    "**Interpretation summary**\n",
    "\n",
    "‚úÖ Model classifies negatives well\n",
    "\n",
    "- Very high specificity (93 %) and low false-positive rate (19 out of 273 negatives).\n",
    "- Most ‚Äú0‚Äù cases are correctly identified.\n",
    "\n",
    "‚ö†Ô∏è Model misses many positives\n",
    "\n",
    "- Low sensitivity or recall (24 %) ‚Äî it only detects about one-quarter of the true positives.\n",
    "- High false-negative rate (97 missed positives).\n",
    "\n",
    "‚öñÔ∏è Overall accuracy = 71 % looks moderate, but this is driven mostly by correctly predicting the majority class (negatives)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460060cc-23da-4683-b174-107557187d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Display with annotations\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0,1])\n",
    "disp.plot(cmap='Blues', values_format='d');  # 'd' formats as integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa29d6a-2fe9-4cc6-8fa7-9f91dcaf1ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_prob, threshold=0.5, labels=[0,1], figsize=(5,4), cmap='Blues'):\n",
    "    \"\"\"\n",
    "    Plot an annotated confusion matrix given predicted probabilities and true labels.\n",
    "    \n",
    "    Parameters:\n",
    "    - y_true: array-like of true binary labels\n",
    "    - y_prob: array-like of predicted probabilities for class 1\n",
    "    - threshold: float, threshold to convert probabilities to 0/1\n",
    "    - labels: list, class labels\n",
    "    - figsize: tuple, figure size\n",
    "    - cmap: str, color map for heatmap\n",
    "    \"\"\"\n",
    "    # Convert probabilities to predicted class\n",
    "    y_pred = (y_prob >= threshold).astype(int)\n",
    "    \n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    cm_percent = cm / cm.sum() * 100  # percentage\n",
    "    \n",
    "    # Combine counts and percentages for annotation\n",
    "    annot = np.array([[\"{}\\n{:.1f}%\".format(cm[i,j], cm_percent[i,j]) for j in range(cm.shape[1])]\n",
    "                      for i in range(cm.shape[0])])\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.heatmap(cm, annot=annot, fmt='', cmap=cmap, xticklabels=labels, yticklabels=labels, cbar=False)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title(f'Confusion Matrix (threshold={threshold})')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a316213-5ec0-4e2b-93e9-30c1e491243a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted probabilities from statsmodels logistic regression\n",
    "y_true = admit['admit']\n",
    "y_prob = result.predict(admit)\n",
    "\n",
    "# Plot at default threshold 0.5\n",
    "plot_confusion_matrix(y_true, y_prob, threshold=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4638b4b2-a666-4b45-98d9-30f7db7149a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_metrics(y_true, y_pred, labels=[0,1]):\n",
    "    \"\"\"\n",
    "    Compute sensitivity, specificity, PPV, and NPV from predicted and true labels.\n",
    "    \n",
    "    Parameters:\n",
    "    - y_true: array-like of true binary labels\n",
    "    - y_pred: array-like of predicted binary labels\n",
    "    - labels: list of class labels [negative, positive]\n",
    "    \n",
    "    Returns:\n",
    "    - metrics: dict with sensitivity, specificity, PPV, NPV\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    TN, FP, FN, TP = cm.ravel()\n",
    "    \n",
    "    sensitivity = TP / (TP + FN)          # True Positive Rate / Recall\n",
    "    specificity = TN / (TN + FP)          # True Negative Rate\n",
    "    ppv = TP / (TP + FP)                  # Positive Predictive Value\n",
    "    npv = TN / (TN + FN)                  # Negative Predictive Value\n",
    "    \n",
    "    metrics = {\n",
    "        'Sensitivity (TPR)': sensitivity,\n",
    "        'Specificity (TNR)': specificity,\n",
    "        'PPV': ppv,\n",
    "        'NPV': npv\n",
    "    }\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ad33e9-2390-42d1-a748-95348e181cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted labels from logistic regression at threshold 0.5\n",
    "y_true = admit['admit']\n",
    "y_pred = (result.predict(admit) >= 0.5).astype(int)\n",
    "\n",
    "metrics = classification_metrics(y_true, y_pred)\n",
    "\n",
    "for k, v in metrics.items():\n",
    "    print(f\"{k}: {v:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4030a089-d11f-4d08-bebe-9a9108833f2b",
   "metadata": {},
   "source": [
    "### üìà ROC Curve Concept\n",
    "\n",
    "The **Receiver Operating Characteristic (ROC)** curve illustrates the trade-off between **True Positive Rate (TPR)** and **False Positive Rate (FPR)** at various classification thresholds.\n",
    "\n",
    "Each point on the curve corresponds to a threshold \\( t \\) used to convert predicted probabilities into binary outcomes.\n",
    "\n",
    "$$\n",
    "\\text{TPR} = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{FPR} = \\frac{FP}{FP + TN}\n",
    "$$\n",
    "\n",
    "- **TPR (Sensitivity / Recall):** proportion of admitted students correctly predicted as admitted.  \n",
    "- **FPR (1 ‚àí Specificity):** proportion of non-admitted students incorrectly predicted as admitted.\n",
    "\n",
    "The **Area Under the Curve (AUC)** represents the probability that a randomly chosen admitted student has a higher predicted probability than a randomly chosen non-admitted student.\n",
    "\n",
    "$$\n",
    "\\text{AUC} = P(\\hat{p}_{admit=1} > \\hat{p}_{admit=0})\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ace23a-3d90-4839-8751-cfef2f460d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve for the UCLA Admissions Dataset\n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "# ROC curve and AUC\n",
    "fpr, tpr, thresholds = roc_curve(admit[\"admit\"], admit[\"pred_prob\"])\n",
    "auc_value = roc_auc_score(admit[\"admit\"], admit[\"pred_prob\"])\n",
    "\n",
    "# Plot ROC\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.plot(fpr, tpr, label=f\"ROC Curve (AUC = {auc_value:.3f})\")\n",
    "plt.plot([0,1], [0,1], \"k--\", label=\"Random Guess\")\n",
    "plt.xlabel(\"False Positive Rate (1 - Specificity)\")\n",
    "plt.ylabel(\"True Positive Rate (Sensitivity)\")\n",
    "plt.title(\"ROC Curve ‚Äî Admissions Logistic Regression\")\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56df7a14-c726-4097-9901-eacd6464ffa3",
   "metadata": {},
   "source": [
    "**Interpretation**\n",
    "\n",
    "- AUC close to 1.0 ‚Üí Excellent discrimination between admitted and non-admitted applicants.\n",
    "- AUC ‚âà 0.5 ‚Üí Model is no better than random guessing.\n",
    "- The shape of the curve helps visualize trade-offs between sensitivity and specificity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7afd400-0e8a-4942-909a-64d670dcdd1c",
   "metadata": {},
   "source": [
    "### Adjusting the Threshold ‚Äî Youden‚Äôs J Statistic\n",
    "\n",
    "Instead of balancing FPR and FNR, we‚Äôll maximize **Youden‚Äôs J statistic**, defined as:\n",
    "\n",
    "$$\n",
    "J = TPR - FPR\n",
    "$$\n",
    "\n",
    "The optimal threshold, $J$ reaches its maximum value:\n",
    "\n",
    "$$\n",
    "J_{\\max} = \\max_{t \\in [0,1]} \\left( \\text{TPR}(t) - \\text{FPR}(t) \\right)\n",
    "$$\n",
    "\n",
    "This finds the cutoff that maximizes the vertical distance between the ROC curve and the 45¬∞ line ‚Äî which represents random classification (where TPR=FPR).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763baa86-438e-4817-8a8e-9b9ca7ed91f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_vals, tpr_vals, thresholds = roc_curve(admit['admit'], admit['pred_prob'])\n",
    "youden_J = tpr_vals - fpr_vals\n",
    "best_idx = np.argmax(youden_J)\n",
    "best_thresh = thresholds[best_idx]\n",
    "print(f'Best threshold by Youden\\'s J: {best_thresh:.3f}')\n",
    "\n",
    "admit['pred_youden'] = (admit['pred_prob'] >= best_thresh).astype(int)\n",
    "cm_youden = confusion_matrix(admit['admit'], admit['pred_youden'])\n",
    "print('Confusion matrix (Youden threshold):')\n",
    "print(cm_youden)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c69da8-4892-4a47-a286-ecabf22fbf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing Youden‚Äôs J\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(fpr_vals, tpr_vals, label='ROC curve')\n",
    "plt.plot([0,1],[0,1],'--',color='gray')\n",
    "plt.scatter(fpr_vals[best_idx], tpr_vals[best_idx], c='red', label=f'Best J = {youden_J[best_idx]:.3f}')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve with Youden Optimal Threshold')\n",
    "plt.legend();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f72915-5cbe-48e8-968c-78a1686cba32",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Visualizing the TPR‚ÄìFPR Trade-off by Threshold\n",
    "\n",
    "Plotting TPR and FPR as a function of the decision threshold makes the classification trade-off very clear for teaching purposes.\n",
    "\n",
    "Here‚Äôs the section you can add to your Jupyter notebook after computing fpr, tpr, and thresholds from roc_curve():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d2751f-37fd-4825-b375-14e3dd9650d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot TPR and FPR with shaded area between curves\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(thresholds, tpr, label='True Positive Rate (TPR)', linewidth=2)\n",
    "plt.plot(thresholds, fpr, label='False Positive Rate (FPR)', linewidth=2)\n",
    "\n",
    "# Shade the area between TPR and FPR\n",
    "plt.fill_between(thresholds, fpr, tpr, color='skyblue', alpha=0.3, label='TPR - FPR')\n",
    "# Plot the vertical line segment connecting FPR and TPR at the best threshold\n",
    "plt.plot(\n",
    "    [thresholds[best_idx], thresholds[best_idx]],\n",
    "    [fpr[best_idx], tpr[best_idx]],\n",
    "    color='red', linewidth=3, label=\"Youden's J (max)\"\n",
    ")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Rate\")\n",
    "plt.title(\"TPR and FPR as Functions of Classification Threshold\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e0b149-a2ea-472f-b580-40312e16551a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### üéØ TPR‚ÄìFPR vs. TPR‚ÄìTNR Trade-off\n",
    "\n",
    "**TPR vs. FPR (ROC curve view)**\n",
    "\n",
    "- This is the canonical trade-off shown in an ROC curve.\n",
    "- Both rates are positively correlated with leniency of threshold: as you lower the threshold, both TPR and FPR increase.\n",
    "- It visualizes how much extra sensitivity you gain at the cost of extra false alarms.\n",
    "- Use this when comparing overall model discrimination (AUC, ROC curves).\n",
    "\n",
    "**TPR vs. TNR (balanced performance view)**\n",
    "\n",
    "- Here you‚Äôre plotting one rate that increases (TPR) and one that decreases (TNR) as threshold changes.\n",
    "- The intersection point of TPR and TNR corresponds to roughly balanced sensitivity and specificity.\n",
    "- This view is more intuitive pedagogically when teaching the idea of ‚Äúbalancing errors‚Äù ‚Äî the trade-off between correctly catching positives and correctly rejecting negatives.\n",
    "- Use this when illustrating the effect of threshold tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e44cfc1-77ca-49da-838e-45e40c8bb87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Example Plot (TPR and TNR vs Threshold)\n",
    "\n",
    "# Compute TNR\n",
    "tnr = 1 - fpr\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(thresholds, tpr, label='TPR (Sensitivity)', linewidth=2)\n",
    "plt.plot(thresholds, tnr, label='TNR (Specificity)', linewidth=2)\n",
    "plt.axvline(thresholds[np.argmax(tpr - fpr)], color='red', linestyle='--', label=\"Youden's J Optimum\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Rate\")\n",
    "plt.title(\"TPR and TNR as Functions of Classification Threshold\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15f8e28-9fec-4157-9264-1fc8fb078b04",
   "metadata": {},
   "source": [
    "\n",
    "| Plot        | Best For                         | Visual Behavior                                                    |\n",
    "| ----------- | -------------------------------- | ------------------------------------------------------------------ |\n",
    "| **TPR‚ÄìFPR** | ROC curve / discrimination       | Both rise together ‚Üí reveals model‚Äôs trade-off curve               |\n",
    "| **TPR‚ÄìTNR** | Threshold tuning / error balance | One rises while the other falls ‚Üí intuitive symmetry and crossover |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143b1ab3-366c-4286-be44-8c38bd5bc7f3",
   "metadata": {},
   "source": [
    "### Practice Exercise 3\n",
    "\n",
    "Fit an augmented logistic regression predicting admit that adds the interaction `gre:gpa`. Test whether the interaction model is significantly better than the baseline model (use a likelihood-ratio test). Finally, interpret a significant interaction in the context of the UCLA admissions example.\n",
    "\n",
    "$$\n",
    "D = -2\\big(\\ell_0 - \\ell_1\\big) = -2\\ell_0 + 2\\ell_1\n",
    "$$\n",
    "\n",
    "where $\\ell_0$ is the Log-Likelihood for the baseline model and $\\ell_1$ is for the interaction model.\n",
    "\n",
    "Under the null hypothesis that the interaction effect is zero, $D \\xrightarrow{approx} \\chi^2_{df}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee927af-6376-4e51-838a-fa55c80d876e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69782718-7093-4f52-8660-98298315af60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2\n",
    "# YOUR CODE HERE\n",
    "\n",
    "D = ___________________\n",
    "\n",
    "print(f\"D: {D:.3f}\")\n",
    "print(f\"chi^2: {chi2.sf(D, 1): .3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7820403e-75b1-456f-b33a-ff6bdb6b9598",
   "metadata": {},
   "source": [
    "If $\\beta_{\\text{gre x gpa}}$ is significant, which is not the case here, we could conclude ‚ÄúThe benefit of increasing GRE score is larger for applicants with lower GPAs (i.e., GRE can compensate somewhat for lower GPA).‚Äù That is, the effect of GRE diminishes as GPA increases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfc8d7b-4946-45f6-88d1-915899b81507",
   "metadata": {},
   "source": [
    "### Short Replication with `scikit-learn`\n",
    "\n",
    "We now replicate the logistic regression using `sklearn.linear_model.LogisticRegression`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd94d1cf-a880-4562-886e-5d1e0fd7b69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "X = admit[['gre','gpa','rank']]\n",
    "y = admit['admit']\n",
    "\n",
    "preprocess = ColumnTransformer(transformers=[\n",
    "    ('rank', OneHotEncoder(drop='first'), ['rank'])\n",
    "], remainder='passthrough')\n",
    "\n",
    "clf = Pipeline([\n",
    "    ('prep', preprocess),\n",
    "    ('logit', LogisticRegression(solver='lbfgs'))\n",
    "])\n",
    "clf.fit(X, y)\n",
    "\n",
    "print('Intercept and coefficients:')\n",
    "print(clf.named_steps['logit'].intercept_, clf.named_steps['logit'].coef_)\n",
    "\n",
    "print('\\nAccuracy:', clf.score(X, y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1d660b-6d33-481d-a2ec-a4fcd4850584",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "- Logistic regression is a foundational method for binary classification, linking predictors to the probability of an outcome.\n",
    "- Interpretation of coefficients in odds ratio terms connects quantitative analysis to substantive meaning.\n",
    "- Threshold tuning is crucial for controlling the trade-off between false positives and false negatives.\n",
    "- Replicating with `scikit-learn` highlights how API design differs between statistical and machine-learning frameworks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c61bd00-edd1-47f4-b621-05d8bae30321",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Wrap Up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83eadf8b-127b-4e3a-8f45-8f25b38a646c",
   "metadata": {},
   "source": [
    "That's all for now.\n",
    "- Please complete the DC course \"Cluster Analysis in Python\" by noon on 10/27.\n",
    "- Submit the in-class exercise notebook by 6:00 PM today."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2b80a3-f6d3-4f94-8583-b5b923493bb9",
   "metadata": {},
   "source": [
    "BY PRINTING YOUR NAME BELOW, YOU CONFIRM THAT THE EXERCISES YOU SUBMITTED IN THIS NOTEBOOK ARE YOUR OWN AND THAT YOU DID NOT USE AI TO ASSIST WITH YOUR WORK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdca0491-b4b8-4a65-bc93-8e72247d24dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRINT YOUR NAME\n",
    "print(\"Enter Your Name Here\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pysocs",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
