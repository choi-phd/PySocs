{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06d02bd7-11b2-43c6-9b5c-906b07a064ba",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Python for Social Science"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282906ef-5ba7-44e7-993f-0c16cab35acc",
   "metadata": {},
   "source": [
    "<img src=\"../figures/PySocs_banner.png\" width=\"50%\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c5a140-8538-41ce-9717-68e7b1da789b",
   "metadata": {},
   "source": [
    "# Pandas Advanced Topics\n",
    "\n",
    "Continuing from our previous class, we will explore a range of common data manipulation techniques in pandas, such as handling missing data, reshaping, pivoting, and merging."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cf9ba0-db67-4af1-8fd5-952c0f6b10b0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Handling Missing Data\n",
    "\n",
    "![Missing Data](../figures/missing_data.png)\n",
    "\n",
    "Missing data is a common issue in real-world data analysis. When dealing with data, it's important to correctly identify, manage, and understand missing values to ensure accurate analysis and interpretation.\n",
    "\n",
    "In this section, we will cover:\n",
    "\n",
    "- Understanding different types of missing data, `None`, `np.nan`, `pd.NA`\n",
    "- Identifying missing data with `isna()`, `isnull()` and `notnull()`  \n",
    "- Removing missing data using `dropna()`  \n",
    "- Filling missing data with `fillna()` \n",
    "- Imputing missing data stochastically"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983d6130-db89-4ecc-b1bd-7fc6734f880b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Missing Data Types\n",
    "\n",
    "- `None`: Native Python null value, often used in **object**-type columns.\n",
    "- `np.nan`: NumPy's \"Not a Number\", used for **float**-type columns.\n",
    "- `pd.NA`: Pandas' newer missing value marker, introduced for better support across data types.\n",
    "\n",
    "For illustration purposes, the dataset below includes all three types of missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae2a7f3-be9f-4b8d-8d92-13bf017485a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = {\n",
    "    'name': ['Alice', 'Bob', None, 'David'],\n",
    "    'age': [25, np.nan, 30, 22],\n",
    "    'income': [50000, 60000, pd.NA, 45000]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2641175a-5503-4680-afba-35cc6c774814",
   "metadata": {
    "tags": []
   },
   "source": [
    "Pandas handles `None`, `np.nan`, and `pd.NA` with subtle but important differences. Here's a breakdown of how each behaves and how Pandas treats them:\n",
    "\n",
    "### üîç Missing Value Types\n",
    "\n",
    "| Value Type | Origin  | Typical Use       | Treated as Missing? | Notes                                |\n",
    "|------------|---------|-------------------|----------------------|--------------------------------------|\n",
    "| `None`     | Python  | Object dtype      | ‚úÖ Yes               | Compatible with object columns       |\n",
    "| `np.nan`   | NumPy   | Float dtype       | ‚úÖ Yes               | Used in numerical columns            |\n",
    "| `pd.NA`    | Pandas  | Nullable dtypes   | ‚úÖ Yes               | Consistent across all data types     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217c5830-2950-4a19-a589-b6b33b488887",
   "metadata": {},
   "source": [
    "`None` is a sentinel value for native Python objects. It is compatible with NumPy arrays and Pandas Series as long as they are of `dtype=object`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18bf73a-4529-4a58-b728-eb0003d1e405",
   "metadata": {},
   "source": [
    "To clarify this point, let's create an NumPy array of integer values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5245a1c8-0296-46ac-94e3-72582db5e55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_arr = np.array([1, 2, 3, 4])\n",
    "num_arr.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a1ad92-af83-4904-a276-880200946ac1",
   "metadata": {},
   "source": [
    "If we attempt to assign `None` to one of the elements, a `TypeError` will occur because `num_arr` is of type 'int64', which cannot store values of type 'object' such as `None`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af105f7a-d4a7-4261-b032-49bf58c9427b",
   "metadata": {
    "tags": []
   },
   "source": [
    "‚ùå: TypeError\n",
    "\n",
    "```python\n",
    "num_arr[2] = None\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ae6a76-dd0e-48c0-a39e-c3c63f0e147d",
   "metadata": {},
   "source": [
    "`np.nan` is NumPy's \"Not a Number\", which is used for **float**-type arrays. Because `num_arr` has already been defined as an integer array, the following will also raise a TypeError."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0934422e-e45e-4dcd-a963-1ca2a2c32d39",
   "metadata": {},
   "source": [
    "‚ùå: TypeError\n",
    "\n",
    "```python\n",
    "num_arr[2] = np.nan\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151e639d-a60a-4afb-89f8-5758ae74af97",
   "metadata": {},
   "source": [
    "We **cannot** use `np.nan` to represent missing values in integer arrays because `np.nan` is a sentinel value for **float**-type. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511a0c17-1a17-491b-ae5d-2b3aa39eb461",
   "metadata": {},
   "source": [
    "### Custom Markers of Missing Data\n",
    "\n",
    "Unfortunately, NumPy arrays with the 'int64' dtype lack a built-in sentinel value to represent missing data.\n",
    "\n",
    "Two common workarounds are: (1) defining the array with a 'float64' dtype, and (2) using a specific integer value, such as `-999`, as a sentinel. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6dfc4b-88a4-4eb0-b9e9-5b353e23b49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the array with a 'float64' dtype\n",
    "num_arr = np.array([1, 2, 3, 4], dtype='float64')\n",
    "num_arr[2] = np.nan\n",
    "num_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38de53b-12d2-4dd9-a2e1-0e4b26fe0e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using a specific integer value as a sentinel\n",
    "num_arr = np.array([1, 2, 3, 4], dtype='int64')\n",
    "num_arr[2] = -999\n",
    "num_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d67a37-1af2-48cd-9f74-e9199472ec0e",
   "metadata": {},
   "source": [
    "If you use the second approach (e.g., `-999`), you should take precautions and carefully document them for others and for your future self to prevent doing something like this inadvertently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dbee48-bdc4-4b9b-b319-0a94688129dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_arr.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43698d90-82ee-435e-9686-0b90ff6ed26d",
   "metadata": {},
   "source": [
    "One way to guard against such glitches is to create a **masked** array instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb550e75-32a8-4e57-a9bf-2f6ee407603d",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.array([1, 2, -999, 4])\n",
    "mask = [0, 0, 1, 0]\n",
    "\n",
    "# alternatively\n",
    "mask = [1 if x == -999 else 0 for x in arr]\n",
    "\n",
    "masked_arr = np.ma.masked_array(arr, mask=mask, dtype='int64')\n",
    "masked_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292a6ee7-644a-499e-9983-b66482c541c9",
   "metadata": {},
   "source": [
    "Explanation of `mask=[0, 0, 1, 0]`:\n",
    "- `0` means not masked (valid data)\n",
    "- `1` means masked (missing or invalid data)\n",
    "\n",
    "So in this case:\n",
    "- `1` ‚Üí valid\n",
    "- `2` ‚Üí valid\n",
    "- `-999` ‚Üí masked (treated as missing)\n",
    "- `4` ‚Üí valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d178f3a1-16c3-4cb3-9706-0e75f85f8d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_arr.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5432288e-4ad6-4265-a69f-7ef31a05f859",
   "metadata": {},
   "source": [
    "If you think that's too much work, the best option for now is to define your array as `float64` instead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854ca718-1c46-4a6f-b84a-7dfcc016e60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "float_arr = np.array([1, 2, np.nan, 4], dtype='float64')\n",
    "float_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350f74fa-9c2e-4f29-aea9-fedbcd45ac0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(float_arr) # or float_arr.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e8cf46-c910-42fc-9784-cc6bac2aa3d3",
   "metadata": {},
   "source": [
    "‚ùó By default, `np.mean()` does not ignore `NaN` values ‚Äî if a `NaN` is present anywhere in the data, the result will propagate `NaN`, making the entire mean calculation return `NaN`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3becbf43-9da0-4d40-aa9a-13001191fce6",
   "metadata": {},
   "source": [
    "To compute the mean of a NumPy array that contains `np.nan` values, you need to use `np.nanmean()` instead of the regular `np.mean()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4148bb-c0ba-462f-97fb-404e0342498a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.nanmean(float_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979f59a5-6806-41bd-8157-2fb191e0522c",
   "metadata": {},
   "source": [
    "Alternatively, you can manually remove `NaN` values before calculating the mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc783d7-f0d3-4784-8f7c-6469992c9a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(float_arr[~np.isnan(float_arr)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b69584f-bbaa-469d-bfd5-0e32b09624eb",
   "metadata": {},
   "source": [
    "Unlike `np.array`, a Pandas Series can handle `np.nan` values by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174e040d-76f8-4c3c-9a8d-cbef337a4047",
   "metadata": {},
   "outputs": [],
   "source": [
    "float_ser = pd.Series([1, 2, np.nan, 4], dtype='float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e20c38-1508-4a9e-b84f-c54f1eda4e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "float_ser.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005e5885-7e0a-4de2-896c-789c996535af",
   "metadata": {},
   "source": [
    "By default, `.mean()` ignores `NaN` values (`skipna=True`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807e850e-6bd3-4e63-9126-268209afafe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "float_ser.mean(skipna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640bb4f8-6aab-4aa6-953a-78527b287f24",
   "metadata": {},
   "source": [
    "### Practice Exercise 1\n",
    "\n",
    "You are given a list of daily temperatures in Celsius:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97caf480-8e63-45d1-a5cf-b2447a373c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "temps_list = [22.5, 23.0, None, 21.5, 20.0, np.nan, 19.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0bb057-e462-41e3-94e5-3c042b5453de",
   "metadata": {},
   "source": [
    "Follow these steps:\n",
    "\n",
    "1. Convert the list to a NumPy array with `dtype='float64'` to properly handle missing values (`None` and `np.nan`).\n",
    "2. Identify any missing values in the array.\n",
    "3. Calculate the average temperature (`mean_temps`), ignoring missing values.\n",
    "4. Replace all missing values with the **mean** value to create a cleaned array (`temps_cleaned`), and display the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d773f3d6-4468-4ad2-a11b-4e3596ab4cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1. Create the NumPy array `temps`\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f685f5-87ef-4654-b9fe-67fcd77f7ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2. Identify missing values `missing_mask` \n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73628635-fb72-4b07-8b2c-291762a1d642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3. Compute the average temperature, ignoring missing values, \n",
    "# as `mean_temps`\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525aef17-83c3-4c84-bc8a-3056b6f401db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4. Replace all missing values with the mean of the non-missing values, \n",
    "# and show the cleaned array, `temps_cleaned`.\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645a23ae-2fe7-455e-a0e3-9953ac7b539c",
   "metadata": {},
   "source": [
    "### Pandas Nullable Dtype\n",
    "\n",
    "More recently, Pandas introduced a newer missing value marker, `pd.NA`, for better support across different data types. For example, you can create a **pd.Series** with all available markers of missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08646949-118e-4ad9-b3fe-77de574a04a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_arr = pd.Series([1, None, np.nan, pd.NA], dtype=\"Int64\")\n",
    "num_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f9594a-93df-4324-998c-d0d73e366078",
   "metadata": {},
   "source": [
    "These *nullable dtypes* are distinguisehd from regular dtypes by capitalization of their names (e.g., `pd.Int64` vs. `np.int64`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ce40eb-56c5-4ba5-97ba-7600e42f55da",
   "metadata": {},
   "source": [
    "### Identifying Missing Data\n",
    "\n",
    "Python doesn't have built-in functions for identifying missing values. However, Pandas provides two functions for checking all missing value types (`None`, `np.nan`, `pd.NA`), namely `pd.isnull()` and `pd.isna()`, and also corresponding methods, e.g., `DataFrame.isnull()` and `DataFrame.isna()`. \n",
    "\n",
    "`pd.isna()` is synomyous to and alias for `pd.isnull()`, and they are functionally equivalent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9e5df1-b740-4650-997d-7befbb33266a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.isnull(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1f177a-46c1-45f9-8d02-eb0ac66c8997",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e9dfe1-996c-4744-b9a2-d3fc0bf3a864",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.notna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbd7aae-8364-4174-9647-98c76492fe96",
   "metadata": {},
   "outputs": [],
   "source": [
    "~df.isna() # equivalent to df.notna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f32411-3ca2-4f0f-bdb4-a27c7e475198",
   "metadata": {},
   "source": [
    "#### Filtering Out Missing Data\n",
    "\n",
    "Dropping missing values is straightforward‚Äîsometimes deceptively so, as it may lead to careless application. To illustrate this, let's create a sample DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6cd35b-1980-4de2-b3a0-7fcade156587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a random seed for reproducibility\n",
    "np.random.seed(2222)\n",
    "\n",
    "# Create a sample data (dictionary) with missing values\n",
    "data = {\n",
    "    'CustomerID': [f'CUST{i:03d}' for i in range(1, 11)],\n",
    "    'Age': [25, 34, np.nan, 45, 29, 38, np.nan, 50, 41, 34],\n",
    "    'AnnualIncome': [50000, 62000, 58000, np.nan, 72000, 69000, 64000, 71000, np.nan, 67000],\n",
    "    'SatisfactionScore': [8, 7, 9, 6, np.nan, 8, 7, np.nan, 6, 9],\n",
    "    'LastPurchaseDate': [\n",
    "        '2023-01-15', '2023-02-20', '2023-03-05', np.nan,\n",
    "        '2023-04-10', '2023-05-25', '2023-06-30', '2023-07-15',\n",
    "        np.nan, '2023-08-20'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Convert LastPurchaseDate to datetime\n",
    "df['LastPurchaseDate'] = pd.to_datetime(df['LastPurchaseDate'], errors='coerce')\n",
    "# errors='coerce' prevents the code from crashing due to unexpected formats\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667e1619-ec77-4d2d-bd81-bef4825efa22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of missing values by column\n",
    "df.isna().sum() # equivalent to df.isna().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c93b0c-b933-4bd0-94ac-962876f57328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of missing values by row\n",
    "df.isna().sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a62b8de-9518-4835-9a9b-924e2b915767",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ‚ö†Ô∏è List-wise deletion\n",
    "df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abd31e5-8ddb-4a87-964a-a68b5a35feb2",
   "metadata": {},
   "source": [
    "By default, `dropna()` will remove all rows that contain any missing values (i.e., it performs list-wise deletion). This is equivalent to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85399d0-774a-4f62-a52f-1c817d61d496",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(how=\"any\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5606f9c9-571b-410c-91a7-91b2c42e78ec",
   "metadata": {},
   "source": [
    "You may want to drop rows with all NA values:\n",
    "\n",
    "```python\n",
    "df.dropna(how=\"all\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5346c3e0-31cc-4e2b-84a2-0610cb21eecd",
   "metadata": {},
   "source": [
    "Alternatively, you may want to set a threshold for **a minimum number** of non-NA values for the row to be kept:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5f5b1d-a296-491a-947a-5413d3562aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(thresh=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d0b1df-801d-4c95-a8c6-eb500180fd2e",
   "metadata": {},
   "source": [
    "`dropna()` also supports the `axis=1` argument, which allows you to apply the previously specified rules to columns. For example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52be5c85-2648-494a-b93d-5ab89796cf16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(how=\"any\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e45d4b4-f7b4-4c5e-9a13-b8f24b1dfe4d",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è This removed all columns containing any missing data, leaving almost no information except for the ID field."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7618bef-c7cc-460c-b684-16dbf393f415",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Filling In Missing Data\n",
    "\n",
    "Instead of dropping rows or columns with missing data, we might want to replace them with specific values. Pandas offers the `fillna` method, which can **easily** fill in the gaps. Be cautious, because you might accidentally introduce incorrect data and undermine your otherwise good results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da1f874-a989-47d8-8ac9-ca7fa18baab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Age'].fillna(999)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda73def-098f-4cb3-b867-29507a3897aa",
   "metadata": {
    "tags": []
   },
   "source": [
    "## üß† Imputing Missing Data\n",
    "\n",
    "Imputation is the process of replacing missing data with substitute values. It is an important step in data cleaning and preprocessing, especially since real-world datasets often have missing values caused by errors, non-responses, censoring, or system limitations.\n",
    "\n",
    "Without imputation, many analytical methods‚Äîsuch as regression, classification, or clustering‚Äîmay fail or yield biased results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd830fdc-6eb0-48ca-9461-09076cd25fb0",
   "metadata": {},
   "source": [
    "#### üîß Common Imputation Techniques\n",
    "\n",
    "- **Mean Imputation**\n",
    "    Replace missing values with the column's mean. Simple and fast, but can distort (decrease) variance.\n",
    "- **Median Imputation**\n",
    "    Use the median instead of the mean. More robust to outliers.\n",
    "- **Mode Imputation**\n",
    "    Replace with the most frequent value. Useful for categorical data.\n",
    "- **Random Imputation (with probabilities)**\n",
    "    Sample from existing values based on their observed frequencies. Helps preserve the original distribution.\n",
    "- **Model-based Imputation**\n",
    "    Use predictive models (e.g., regression, k-NN, or machine learning) to estimate missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb64eace-7525-4360-a37e-5e23ec80426b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean imputation\n",
    "\n",
    "df['Age_inp'] = df['Age'].fillna(df['Age'].mean())\n",
    "df[['Age', 'Age_inp']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cefa141-7d8d-4570-a28e-27bb1f1ec24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Median imputation\n",
    "\n",
    "df['Age'].fillna(df['Age'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48f0da6-d3d0-4f46-86cf-2af4d4c4100d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['Age'].mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d1435b-8266-4548-8695-2cb881561737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mode imputation\n",
    "\n",
    "df['Age'].fillna(df['Age'].mode()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53272755-4fb2-4a3b-97a9-ce658d717c74",
   "metadata": {},
   "source": [
    "Subsetting the output of `mode()` using `mode()[0]` is necessary because the function can return an array instead of a scalar value when there are ties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d2220a-4c75-4441-be40-6580a5a71abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random imputation\n",
    "probs = df['SatisfactionScore'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717c7d9c-82d8-49f5-9743-0cf21bf59f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f976757-8a0c-42c8-b8c2-f3b937372364",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb761441-2646-4c8c-b1d1-d761d9f2fbff",
   "metadata": {},
   "source": [
    "This function `random_impute()` implements a random imputation based on the observed probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9efa2a5-69a6-440e-beb9-cc535be6f3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_impute(arr):\n",
    "    probs = arr.value_counts(normalize=True)\n",
    "    return arr.apply(lambda x: x if pd.notnull(x) else np.random.choice(probs.index, p=probs.values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97816e0b-2595-4b48-bddd-91c5e8e1d79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SatisfactionScore_inp'] = random_impute(df['SatisfactionScore'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f3394a-0e00-4b57-8c5f-00d725351e31",
   "metadata": {},
   "source": [
    "Replacing missing values in this manner is acceptable in certain situations, particularly when the proportion of missing data is relatively small (e.g., less than 5%). However, more advanced methods exist that account for and properly represent the uncertainty of imputed values and their conditional dependency on other variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc6e2a8-5225-4b05-9b5c-d9f014539a97",
   "metadata": {},
   "source": [
    "### Practice Exercise 2\n",
    "\n",
    "For this exercise, we will download the census income dataset (`adult.data`) from the UCI Machine Learning Repository. We'll select a subset of the data and artificially add missing values to some continuous or ordinal variables for imputation practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6de7ebbd-f30f-469a-8f6b-01ff5c00842d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 32561 entries, 0 to 32560\n",
      "Data columns (total 15 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   age             32561 non-null  int64 \n",
      " 1   workclass       32561 non-null  object\n",
      " 2   fnlwgt          32561 non-null  int64 \n",
      " 3   education       32561 non-null  object\n",
      " 4   education_num   32561 non-null  int64 \n",
      " 5   marital_status  32561 non-null  object\n",
      " 6   occupation      32561 non-null  object\n",
      " 7   relationship    32561 non-null  object\n",
      " 8   race            32561 non-null  object\n",
      " 9   sex             32561 non-null  object\n",
      " 10  capital_gain    32561 non-null  int64 \n",
      " 11  capital_loss    32561 non-null  int64 \n",
      " 12  hours_per_week  32561 non-null  int64 \n",
      " 13  native_country  32561 non-null  object\n",
      " 14  income_gt_50k   32561 non-null  object\n",
      "dtypes: int64(6), object(9)\n",
      "memory usage: 3.7+ MB\n"
     ]
    }
   ],
   "source": [
    "# Download `adult.data` from UCI ML repo or via URL\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
    "cols = [\n",
    "    \"age\", \"workclass\", \"fnlwgt\", \"education\", \"education_num\", \"marital_status\",\n",
    "    \"occupation\", \"relationship\", \"race\", \"sex\", \"capital_gain\", \"capital_loss\",\n",
    "    \"hours_per_week\", \"native_country\", \"income_gt_50k\"\n",
    "]\n",
    "\n",
    "df = pd.read_csv(url, header=None, names=cols, na_values=\" ?\", skipinitialspace=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfa8773-de15-4599-8184-2f869b026c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering\n",
    "df_sample = df.loc[df[\"native_country\"] == 'United-States']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6b5c23-6f65-4449-95b3-90944e6fd497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subsetting\n",
    "rng = np.random.default_rng(seed=2222)\n",
    "\n",
    "df_sample = df.sample(n=1000)\n",
    "df_sample_imp = df_sample.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dafac2-61b2-4172-a47e-8460f8a6ced5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introducing missing data\n",
    "# 8% missing in age, 5% in hours_per_week, 10% in capital_gain, 10% in capital_loss\n",
    "for col, pct in [(\"age\", 0.08), (\"hours_per_week\", 0.05), (\"capital_gain\", 0.10), (\"capital_loss\", 0.10)]:\n",
    "    mask = rng.random(len(df_sample_imp)) < pct\n",
    "    df_sample_imp.loc[mask, col] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89dff018-081d-45ba-84b1-3919851dec40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting NA\n",
    "df_sample_imp.isna().sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1363ee9e-3cf8-415e-9b81-b7942d892a8f",
   "metadata": {},
   "source": [
    "**Task A**. Calculate and print the mean and median for the variables `age`, `hours_per_week`, `capital_gain`, and `capital_loss`, excluding any missing values. Begin by listing the columns you want to analyze, and store the resulting statistics in arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc667321-2c5f-4ef0-91cb-8e2511a50cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284fe67c-b120-4103-8a00-771a62bc8060",
   "metadata": {},
   "source": [
    "**Task B**. Impute missing values in:\n",
    "\n",
    "- `age` and `hours_per_week` using **mean** imputation\n",
    "- `capital_gain` and `capital_loss` using **median** imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777f0192-0839-41fc-82e3-0b6898ec03a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa5c64c-1ce0-49f6-8377-bc1fb7216d0d",
   "metadata": {},
   "source": [
    "**Task C**. After imputation, re-check that there are no missing values and compute descriptive statistics (mean, median, std) for all columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa950982-bbe8-4524-8bd9-fe032793e1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48ca7cb-8e1a-4364-b180-e3b487829324",
   "metadata": {},
   "source": [
    "**Task D**. Compare how the distribution of the imputed columns, `cols`, changes before vs. after imputation. Hint: `describe()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c798811f-c14e-44a5-b888-7b59f5272315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d0acff-f0ef-4259-a7ba-c699f76a9745",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Multiple Imputation\n",
    "\n",
    "Multiple Imputation (MI) is a statistical technique for handling missing data by creating several plausible datasets, analyzing each one, and then combining the results.\n",
    "\n",
    "| **Single Imputation**                                | **Multiple Imputation**                                                    |\n",
    "| ---------------------------------------------------- | -------------------------------------------------------------------------- |\n",
    "| Fill missing values once (e.g., mean, median, mode). | Create *M* datasets with different plausible values for each missing cell. |\n",
    "| Ignores uncertainty ‚Üí **underestimates variance**.   | Captures uncertainty ‚Üí **more realistic statistical inference**.           |\n",
    "| Easy but **biased estimates** possible.              | More robust, theoretically sound.                                          |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5267af7-af71-45dd-a069-a16a8b4eaeec",
   "metadata": {},
   "source": [
    "### Why Imputation is Needed\n",
    "\n",
    "Real-world datasets often contain missing values due to:\n",
    "- Non-responses in surveys\n",
    "- Data entry errors\n",
    "- Merge mismatches\n",
    "\n",
    "Many statistical models (e.g., linear regression, logistic regression, tree-based models) cannot handle missing values directly, so we must fill (impute) them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a812fe-b69f-42a9-ba9b-f9cb441fdb1a",
   "metadata": {},
   "source": [
    "### Workflow of Multiple Imputation\n",
    "\n",
    "- **Imputation step**: \n",
    "Generate several complete datasets by filling missing values with random draws from predictive distributions.\n",
    "\n",
    "- **Analysis step**: \n",
    "Perform the desired analysis on each completed dataset separately.\n",
    "\n",
    "- **Pooling step**: \n",
    "Combine results using Rubin's Rules to get final parameter estimates and standard errors.\n",
    "\n",
    "![Source: ](../figures/MI.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0e5fde-fc8a-419f-9706-8414698123d8",
   "metadata": {},
   "source": [
    "### Python Libraries for Multiple Imputation\n",
    "\n",
    "| Library        | Method Type                                     | Notes                          |\n",
    "| -------------- | ----------------------------------------------- | ------------------------------ |\n",
    "| `scikit-learn` | Iterative Imputer (MICE-like)                   | Good for predictive modeling   |\n",
    "| `statsmodels`  | MICE (Multiple Imputation by Chained Equations) | Classical statistical approach |\n",
    "| `fancyimpute`  | Advanced ML-based imputation                    | Deep learning, KNN             |\n",
    "| `miceforest`   | Random forest-based MI                          | Very flexible, robust          |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ee9f8a-d06f-40c3-9c48-b46ea164d7a2",
   "metadata": {},
   "source": [
    "### Example Dataset: Customer Survey\n",
    "\n",
    "Let's simulate a dataset (N=100) with missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35e2760-5f23-44bb-914d-98ca39137681",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(2222)\n",
    "\n",
    "# Create dataset\n",
    "data = pd.DataFrame({\n",
    "    'Age': np.random.randint(18, 65, 100),\n",
    "    'Income': np.random.normal(55000, 15000, 100).round(0),\n",
    "    'SpendingScore': np.random.randint(1, 100, 100),\n",
    "    'Satisfaction': np.random.choice(['Low', 'Medium', 'High'], 100)\n",
    "})\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951a6413-240f-48e4-b8e4-869069e69efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6448669a-5258-42f1-9512-fde4436861f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Satisfaction'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5d2833-0109-4475-ad03-af170a5b2e8d",
   "metadata": {},
   "source": [
    "We will randomly select 10% of the `Age` data and replace them with `np.nan`. Similarly, 15% of `Income` and 5% of `SpendingScore` data will be randomly replaced with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf6b806-27bf-4bd4-803d-a1059b84055a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introduce missingness\n",
    "data.loc[data.sample(frac=0.10).index, 'Age'] = np.nan\n",
    "data.loc[data.sample(frac=0.15).index, 'Income'] = np.nan\n",
    "data.loc[data.sample(frac=0.05).index, 'SpendingScore'] = np.nan\n",
    "\n",
    "print(\"Missing value counts:\\n\")\n",
    "print(data.isnull().sum(), \"\\n\")\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de32c64b-d0c2-4762-bf9e-1043a948cf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371b527e-86db-4010-809f-462a6a29220c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Satisfaction'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5dbb1af-b0b9-41e7-ad1b-888838c9673e",
   "metadata": {},
   "source": [
    "#### Visualizing Missing Values\n",
    "\n",
    "We can visualize the missing data patterns using a heatmap. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc06644-4c1b-4b7f-b9e5-172a56ae4987",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def missing_pattern(df):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(df.isnull(), cbar=False, cmap=\"Oranges\", yticklabels=False)\n",
    "    plt.title(\"Missing Data Heatmap\")\n",
    "    plt.show()\n",
    "    \n",
    "missing_pattern(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e136234-c4b5-4f3f-a2b2-ee36f9529397",
   "metadata": {},
   "source": [
    "#### MICE with `miceforest`\n",
    "\n",
    "Multiple Imputation by Chained Equations (MICE) addresses missing data in a dataset by repeatedly running predictive models that estimate and fill in the missing values. During each cycle, each chosen variable is imputed based on the information from all other variables. This process is repeated until the results suggest convergence has been achieved.\n",
    "\n",
    "![miceforest](../figures/miceforest.png)\n",
    "\n",
    "Source: https://pypi.org/project/miceforest/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c836dfb-8e27-40e5-bf2d-db4de2273252",
   "metadata": {},
   "source": [
    "#### Data Pre-Processing\n",
    "\n",
    "One retriction in running `miceforest` is that you cannot directly impute object columns.\n",
    "\n",
    "It will raise an error if your input DataFrame contains columns with object dtype (e.g., strings or mixed types), as `miceforest` only supports Numeric types (int, float) and Categorical types (category in pandas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2616e23d-59da-4a03-ba3a-341252b3c8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8fff75-c283-4799-9fa3-59f162912426",
   "metadata": {},
   "source": [
    "We can convert `Satisfaction` to a categorical variable as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94dda14a-0954-4e62-9567-c3ca9dce4c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Satisfaction'] = data['Satisfaction'].astype('category')\n",
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cad675-f97c-4a7e-975c-9b8743c8e139",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isna().sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0140d265-5c4d-49bd-b65f-3128c2411908",
   "metadata": {},
   "source": [
    "When imputing a variable with missing data (say, \"Age\"), MICEforest:\n",
    "- Treats \"Age\" as the target variable.\n",
    "- Uses all the other variables in the dataset as predictors (\"Income\" and \"SpendingScore\").\n",
    "- Trains a random forest model to predict the missing values of \"Age\" based on the observed data.\n",
    "\n",
    "For example:\n",
    "\n",
    "If \"Age\" has missing values:\n",
    "- MICEforest builds a random forest model:\n",
    "\n",
    "$$Age ‚àº Education + Income + SpendingScore$$\n",
    "\n",
    "- It uses rows where \"Age\" is NOT missing to train the model.\n",
    "- Then, it predicts \"Age\" for rows where it is missing.\n",
    "\n",
    "This process is repeated for every column with missing data, one variable at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f825fa47-3c5b-4959-a1d7-57c67b61aabc",
   "metadata": {},
   "source": [
    "**Step 0** ‚Äì Initialization\n",
    "\n",
    "- All missing values are temporarily filled in using a simple method:\n",
    "    - Mean imputation, median imputation, or random sampling.\n",
    "- This gives you a complete dataset to start modeling, even though the initial imputations are crude."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000218ae-6b64-42d8-a474-f47088353f55",
   "metadata": {},
   "source": [
    "**Step 1** ‚Äì First Iteration\n",
    "\n",
    "MICE now goes column-by-column, imputing missing values:\n",
    "\n",
    "1. Impute Age\n",
    "    - Model: Age ~ Income + SpendingScore\n",
    "    - Uses current versions of Income and Education (including their temporary imputed values).\n",
    "\n",
    "2. Impute Income\n",
    "\n",
    "- Model: Income ~ Age + SpendingScore\n",
    "- Uses updated Age and current SpendingScore.\n",
    "\n",
    "3. Impute SpendingScore\n",
    "\n",
    "- Model: SpendingScore ~ Age + Income\n",
    "- Uses updated versions of Age and Income.\n",
    "\n",
    "At the end of **Step 1**, all variables have new, better imputations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6dfc6a-79e1-4e5a-b623-4e2419dfa713",
   "metadata": {},
   "source": [
    "**Step 2** ‚Äì Repeat\n",
    "\n",
    "- The process repeats again, now using the updated imputations as predictors.\n",
    "- With each iteration, the imputations become more consistent, as each variable's prediction benefits from improved values of the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c0c93a-3f2d-4528-b20a-0c731db9a29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import miceforest as mf\n",
    "\n",
    "# Create a kernel and run multiple imputations\n",
    "kernel = mf.ImputationKernel(\n",
    "    data,\n",
    "    num_datasets=4,\n",
    "    save_all_iterations_data=True,\n",
    "    random_state=2222\n",
    ")\n",
    "\n",
    "# Run the MICE algorithm for 5 iterations\n",
    "kernel.mice(5)\n",
    "\n",
    "# Printing the kernel will show you some high level information.\n",
    "print(kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc02bf5-f168-41e2-b04f-f0c4af0c98cd",
   "metadata": {},
   "source": [
    "After we have run mice, we can obtain our completed dataset directly from the kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b0de11-14aa-436c-a538-3fc4bb2dc6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9886c4e3-e065-4ee5-965c-399e0ac7a7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_imp_0 = kernel.complete_data(dataset=0) # 0 = first imputed dataset\n",
    "\n",
    "missing_pattern(data_imp_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ce8cee-7487-46f4-9d84-367ea71571d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_imp_0.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9af51ab-0bce-4bcc-bff3-88d1c3858a14",
   "metadata": {},
   "source": [
    "Performing statistical analyses on multiple imputed datasets requires concatenating (stacking) all the imputed sets into a single DataFrame. We will cover this topic in more detail later in the semester. For now, we will demonstrate how to prepare a concatenated DataFrame in long format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50e5d91-7e1e-4ab8-a96a-9814ee9f4028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack all imputed datasets into a single DataFrame\n",
    "data_imp_all = pd.concat(\n",
    "    [kernel.complete_data(dataset=i).assign(_imputation_=i) \n",
    "     for i in kernel.datasets],\n",
    "    ignore_index=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beddb53a-c24e-427e-9e66-533775e25132",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_imp_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab613f0-3f97-4ddd-9100-68e04b75b184",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_imp_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432d4b37-d661-48ef-995e-17c4a29137ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_imp_all.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f496a54-8897-4d1a-b508-43dcd56cb2d2",
   "metadata": {},
   "source": [
    "Note that we use `pd.concat()` with a list comprehension because there isn't a built-in function in `miceforest` that combines all imputed datasets into a single DataFrame, similar to R‚Äôs `mice::complete(m=\"long\")`.\n",
    "\n",
    "To streamline this process for repeated use, we can create a custom function and add it to our toolbox:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6a6de6-f868-4935-825e-464e11eafd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_imp(kernel):\n",
    "    \"\"\"Combine all imputed datasets from miceforest kernel into one DataFrame.\"\"\"\n",
    "    return pd.concat(\n",
    "        [kernel.complete_data(dataset=i).assign(_imputation_=i) \n",
    "         for i in kernel.datasets],\n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "# Usage\n",
    "data_imp_all = combine_imp(kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3f4e7e-c0b6-4646-bd36-9247b2ae24d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_imp_all[\"_imputation_\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b71aa7b-f4dd-418f-acaf-a49e07994bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_imp_all.groupby(\"_imputation_\").describe().round(1).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ea66b5-ad84-461d-a964-50bbda90c17d",
   "metadata": {},
   "source": [
    "### Practice Exercise 3\n",
    "\n",
    "Perform multiple imputation on the data using `miceforest` and the following steps:\n",
    "- Generate three (3) imputed datasets.\n",
    "- Run the algorithm with three (3) iterations.\n",
    "- Set the random seed to 2222.\n",
    "- Stack the imputed datasets vertically to create `df_inp_combined`.\n",
    "- For each imputation, produce descriptive statistics for the variable of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b574ffac-3369-4a50-afce-73b0bfed7b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example dataset\n",
    "df = pd.DataFrame({\n",
    "    'Age': [25, np.nan, 30, 35, np.nan, 22, 26, 31, np.nan, 29],\n",
    "    'BMI': [22.5, 24.0, np.nan, 27.0, 26.5, 21.9, 25.2, np.nan, 23.0, 24.5]\n",
    "})\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35baaef7-da34-4d21-8123-e5adb5c471dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abb34cf-cd55-4129-b764-1d885302b373",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Hierarchical Indexing\n",
    "\n",
    "Hierarchical indexing (also called MultiIndex) allows you to have multiple levels of indices in your DataFrame or Series. This is useful for working with higher-dimensional data in a 2D structure. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d1dea1-2396-4772-bc42-fe74b9092215",
   "metadata": {},
   "source": [
    "üìä State Population Estimates\n",
    "\n",
    "| State          | July 1, 2014 Population | July 1, 2024 Population |\n",
    "| -------------- | ----------------------- | ----------------------- |\n",
    "| **California** | 38,680,810              | 39,431,263              |\n",
    "| **New York**   | 19,378,124              | 19,867,248              |\n",
    "| **Texas**      | 26,094,422              | 31,290,831              |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400952dc-6f22-4118-8962-48a0711d9dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define index\n",
    "index = [\n",
    "    ('California', 2014),\n",
    "    ('California', 2024),\n",
    "    ('New York', 2014),\n",
    "    ('New York', 2024),\n",
    "    ('Texas', 2014),\n",
    "    ('Texas', 2024)\n",
    "]\n",
    "\n",
    "# State population estimates\n",
    "population_values = [38_680_810, \n",
    "                     39_431_263, \n",
    "                     19_378_124, \n",
    "                     19_867_248, \n",
    "                     26_094_422, \n",
    "                     31_290_831]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c04183f-5ea9-4c12-9e48-dcd17012ac85",
   "metadata": {},
   "source": [
    "You may consider using the tuples as defined in `index` as keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d8e992-fc26-4d54-be28-eaa4544e05e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_population = pd.Series(population_values, index=index)\n",
    "state_population"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7614f6be-4b74-455f-a17d-53c18f9a744d",
   "metadata": {},
   "source": [
    "With this indexing scheme, you can select a specific (state, year) pair or slice the data accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f300f2fe-3089-4481-9ac5-5b9bafac6879",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_population[(\"Texas\", 2014)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30e4bd8-3b01-4183-a52a-b7920beffa68",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_population[(\"Texas\", 2014):(\"Texas\", 2024)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19aabf63-a3f3-447b-9da9-23db4d7b5c0f",
   "metadata": {},
   "source": [
    "However, selecting all population estimates for 2023 (or all population estimates for Texas) is no longer straightforward. You can still retrieve the data, but the process is now more complicated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b86281a-803e-4475-9932-840ab0ba3196",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_population[[i for i in state_population.index if i[1] == 2014]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12df4d42-be27-4f1b-ab0a-b9751515f5ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "state_population[[i for i in state_population.index if i[0] == \"Texas\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0c708a-7d77-41f1-a56f-60b7f7e1c49b",
   "metadata": {},
   "source": [
    "Pandas offers a simpler method for indexing hierarchically structured data points: `pd.MultiIndex`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5b7dc1-2f9a-48a6-88f6-6dc73b0e7382",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_index = pd.MultiIndex.from_tuples(index, names=['State', 'Year'])\n",
    "multi_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098c80fd-47dc-4a8c-80e0-8fec611b782b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Series\n",
    "state_population = pd.Series(population_values, index=multi_index)\n",
    "state_population"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ebffe4-1ba4-43df-8b99-8660b5418147",
   "metadata": {},
   "source": [
    "Now, we can access all data for Texas as easily as this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9851dd-8b39-4043-91bf-f9302db4e02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_population[\"Texas\", :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77227774-cb87-4bf7-b354-a44fbc1c88aa",
   "metadata": {},
   "source": [
    "Or, all data for 2024:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9612c753-fa25-4251-8d7a-1b38230ccf63",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_population[:, 2024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb2cef5-edf6-4022-bca7-0852e2c37bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_population_state = state_population.unstack()\n",
    "state_population_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbf60d1-94dc-4e54-8388-03a6abe2e2e5",
   "metadata": {},
   "source": [
    "We can also choose which level to \"unstack\" using the level parameter in `unstack()`. It can be either the level name or the level number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816c1d5c-2960-407d-b92f-f91cc5732fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_population_year= state_population.unstack(level=\"State\") # (level=0)\n",
    "state_population_year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1de51b-991a-4815-b5ed-48b544f4d2b9",
   "metadata": {},
   "source": [
    "You can easily undo \"unstack\" with `stack()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a5333c-c024-4837-b23a-ffd3e6ebe122",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_population_state.stack()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b000d994-24bf-4ebd-8dde-62060b86acaf",
   "metadata": {},
   "source": [
    "We can create a new variable `Gain` and then stack:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56471ded-dabe-480b-ae8f-229fe375aaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_population_state['Gain'] = state_population_state[2024] - state_population_state[2014]\n",
    "state_population_state.stack()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca4d157-431a-4f6b-824e-b7f3455409ff",
   "metadata": {},
   "source": [
    "For more details, see Chapter 17 of VanderPlas (2023), pages 132-144."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0bd8b5-0f24-4ef0-96bf-997fa1fa734e",
   "metadata": {},
   "source": [
    "## Pivot Tables\n",
    "\n",
    "Pivot tables are one of the most powerful tools in Pandas for summarizing and aggregating data. They allow you to aggregate, group, and reshape your data in a single step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fba9c4-8182-4a2c-abc2-3c6262b730ad",
   "metadata": {},
   "source": [
    "#### 1. What is a Pivot Table?\n",
    "\n",
    "A pivot table:\n",
    "- Summarizes data by grouping rows and aggregating values.\n",
    "- Similar to pivot tables in Excel but more flexible.\n",
    "- Great for multi-dimensional summaries like:\n",
    "    - Average age by gender and passenger class\n",
    "    - Total sales by region and quarter\n",
    "    - Count of events per category and time period"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24e2074-67c1-43b0-b152-e9268faff967",
   "metadata": {},
   "source": [
    "**Basic syntax**:\n",
    "\n",
    "```python\n",
    "pd.pivot_table(\n",
    "    data, \n",
    "    values=None,      # column(s) to aggregate\n",
    "    index=None,       # row grouping\n",
    "    columns=None,     # column grouping\n",
    "    aggfunc='mean'    # aggregation function: mean, sum, count, etc.\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e573bb-59f7-4b80-b288-b75590fb3741",
   "metadata": {},
   "source": [
    "#### 2. Loading the Titanic Dataset\n",
    "\n",
    "We will use the Titanic dataset, which contains information about passengers on the Titanic:\n",
    "\n",
    "- `survived`: Whether the passenger survived (1) or not (0)\n",
    "- `pclass`: Passenger class (1 = First, 2 = Second, 3 = Third)\n",
    "- `sex`: Gender (female, male)\n",
    "- `age`, `fare`: Numerical values\n",
    "- `embarked`: Port of embarkation (C, Q, S)\n",
    "- etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a67effe-1e7a-463b-95c3-3daf9feca1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = sns.load_dataset('titanic')\n",
    "titanic.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22aa2d60-0ae0-4f69-a3ba-0911cf922bac",
   "metadata": {},
   "source": [
    "#### 3. Simple Pivot Table: Survival Rate by Gender\n",
    "\n",
    "Goal: Find the survival rate of male and female passengers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33e4dd1-dcd4-47ce-86a8-281c54448f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.pivot_table(index=['sex'], values=['survived'], aggfunc='mean').round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bd150a-5a7a-49de-a926-687300d529df",
   "metadata": {},
   "source": [
    "#### 4. Adding a Second Grouping Level\n",
    "\n",
    "Goal: Survival rate by gender and passenger class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7a348e-f65f-4ba3-a120-3ca94cfa4f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.pivot_table(\n",
    "    values='survived',\n",
    "    index='sex',\n",
    "    columns='pclass',\n",
    "    aggfunc='mean'\n",
    ").round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98598814-ff9b-40bc-8893-d477f95fbdda",
   "metadata": {},
   "source": [
    "#### 5. Adding Marginal Means\n",
    "\n",
    "Goal: Survival rate by gender and passenger class, adding the `All` values without taking into account gender/passenger class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f5eae7-fa02-426c-af8d-f64645d4c52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.pivot_table(\n",
    "    values='survived',\n",
    "    index='sex',\n",
    "    columns='pclass',\n",
    "    aggfunc='mean',\n",
    "    margins=True\n",
    ").round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1be420d-16b3-4a8f-9379-58febf131eff",
   "metadata": {},
   "source": [
    "#### 6. Multiple Values\n",
    "\n",
    "Goal: Survival rate and average fare by gender and passenger class, adding the `All` values without taking into account gender/passenger class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528d7e20-2920-4dca-9095-35cd92308f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.pivot_table(\n",
    "    values=['survived', 'fare'],\n",
    "    index='sex',\n",
    "    columns='pclass',\n",
    "    aggfunc='mean',\n",
    "    margins=True\n",
    ").round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786bb50e-a444-40fd-9c26-e8a70eb82b83",
   "metadata": {},
   "source": [
    "#### Adding Age after Binning\n",
    "\n",
    "We can include the `age` variable as another index. Since it is currently a continuous variable, we can divide it into discrete intervals, or \"bins,\" for analysis: (0, 20], (20, 30], (30, 40], (40, 50], and (50, 100]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2252ec3-0e2c-4f13-839d-173ac1f69160",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [0, 20, 30, 40, 50, 100]\n",
    "titanic['age_category'] = pd.cut(titanic.age, bins)\n",
    "titanic['age_category'].value_counts(sort=False) #By default, Series.value_counts() sorts by counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbc633b-abc0-47d7-ba9b-eab57419202e",
   "metadata": {},
   "source": [
    "üîß Now, we can create a nested index using both `sex` and `age_category` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61f1c99-ac37-44a1-beb9-1c2942e46295",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.pivot_table(\n",
    "    values=['survived'],\n",
    "    index=['sex', 'age_category'],\n",
    "    columns='pclass',\n",
    "    aggfunc=['count', 'mean'],\n",
    "    observed=False\n",
    ").round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e187933e-82f0-4760-bcbc-3088c574de56",
   "metadata": {},
   "source": [
    "![Combining DataFrames](../figures/join.png)\n",
    "\n",
    "## Combining and Joining DataFrames\n",
    "\n",
    "In real-world data analysis, information is often spread across multiple datasets. For example:\n",
    "- A student dataset might store student demographic and test score details,\n",
    "- A teacher dataset might contain teacher information, and\n",
    "- A school dataset might contain school details.\n",
    "\n",
    "To analyze these datasets together, Pandas provides powerful tools to combine, merge, and join them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6665ec2d-6492-424f-b39a-6254915daff1",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 1. Why Combine DataFrames?\n",
    "\n",
    "Typical use cases:\n",
    "- **Enriching data**: Adding student demographics to test score records.\n",
    "- **Cleaning data**: Combining multiple district or school files or reports into a single table.\n",
    "- **Analyzing relationships**: Comparing variables from different sources.\n",
    "\n",
    "Pandas supports several methods for combining DataFrames:\n",
    "\n",
    "| Method                      | Use Case                                     |\n",
    "| --------------------------- | -------------------------------------------- |\n",
    "| `pd.concat()`               | Stacking datasets vertically or horizontally |\n",
    "| `df.merge()` / `pd.merge()` | SQL-style joins (inner, outer, left, right)  |\n",
    "| `df.join()`                 | Simplified joins using indexes               |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ad8af2-48eb-4016-b7bf-ededdfa47869",
   "metadata": {},
   "source": [
    "#### Example Datasets\n",
    "\n",
    "For illustration, we will work with these sample datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24cce7d-1ada-4155-b7e9-7b03f0b91ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Student datasets\n",
    "students = pd.DataFrame({\n",
    "    'student_id': [1, 2, 3, 4, 5],\n",
    "    'name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Emily'],\n",
    "    'grade': [3, 3, 4, 4, 4]\n",
    "})\n",
    "\n",
    "demog = pd.DataFrame({\n",
    "    'student_id': [4, 3, 2, 1],\n",
    "    'age': [10, 9, 8, 8],\n",
    "    'sex': ['F', 'M', 'M', 'F']\n",
    "})\n",
    "\n",
    "students, demog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9277f9e8-f506-44f9-b25a-7439da7f2cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test datasets\n",
    "scores_fall = pd.DataFrame({\n",
    "    'test_id': [\"Fall\"] * 4,\n",
    "    'student_id': [2, 1, 3, 4],\n",
    "    'score': [250, 450, 300, 200]\n",
    "})\n",
    "\n",
    "scores_winter = pd.DataFrame({\n",
    "    'test_id': [\"Winter\"] * 5,\n",
    "    'student_id': [2, 1, 3, 4, 5],\n",
    "    'score': [270, 460, 320, 210, 510]\n",
    "})\n",
    "\n",
    "scores_spring = pd.DataFrame({\n",
    "    'test_id': [\"Spring\"] * 5,\n",
    "    'student_id': [1, 3, 4, 5, 6],\n",
    "    'score': [470, 360, 220, 500, 410]\n",
    "})\n",
    "\n",
    "scores_fall, scores_winter, scores_spring\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872b94c3-912c-46a4-a70b-61394f8a0bfa",
   "metadata": {},
   "source": [
    "### Concatenation (`pd.concat`)\n",
    "\n",
    "pd.concat() is used to stack DataFrames:\n",
    "- Vertical stack (rows) ‚Üí similar to adding records from another file.\n",
    "- Horizontal stack (columns) ‚Üí similar to adding new variables or attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617d1317-6120-48e7-8602-37e012770aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vertical Concatenation\n",
    "# Combining `scores_fall` and `scores_spring`\n",
    "\n",
    "scores = pd.concat([scores_fall, scores_winter, scores_spring], ignore_index=True)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4322b262-933d-4d8f-8b24-8a3d7c7f11c9",
   "metadata": {},
   "source": [
    "**Note**: `ignore_index=True` resets the index after concatenation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4633a89f-cb4c-44bd-86ff-fb5737868313",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.groupby('test_id')['score'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33169fa-df25-4102-91c7-644d9caa61ce",
   "metadata": {},
   "source": [
    "By default, `groupby()` in pandas sorts the group keys alphabetically (or numerically).\n",
    "\n",
    "You can keep the original order of appearance by setting the parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9ec48f-6daf-496d-a557-890f73dcb0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.groupby('test_id', sort=False)['score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62823a7-ef6f-4a05-a523-1f3ff2645025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Horizontal concatenation\n",
    "# Adding new columns from a separate datasets\n",
    "# ‚ö†Ô∏è This is problematic!\n",
    "\n",
    "pd.concat([students, demog], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad2ba24-f3dc-4685-86cb-f10758b4e205",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è`pd.concat(axis=1)` aligns records based solely on their **index**, not by columns as `pd.merge()` does. \n",
    "\n",
    "While you can use `reindex` to manually align records, it is generally better to use `merge()` or `join()` when you need to align data on key columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc00c44-7f87-48a5-b49c-485a3b27ded1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([students, demog.reindex([3, 2, 1, 0]).reset_index(drop=True)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0625b745-ef59-4d20-86d2-e0be45b05719",
   "metadata": {},
   "source": [
    "### Merging DataFrames (`merge`)\n",
    "\n",
    "The `merge()` function works like SQL joins by combining datasets based on one or more keys.\n",
    "\n",
    "Let's repeat the previous horizontal concatenation example, but this time we'll use `merge()` with `student_id` as the key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c84b45-c032-446d-b055-32d41392eb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inner Join\n",
    "pd.merge(students, demog, on='student_id', how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddea4f64-c16f-4e7d-a01d-f6011cf544d3",
   "metadata": {},
   "source": [
    "Only students who have demographic information are retained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fd701d-c9bc-4d31-a49b-8edf455cfe1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outer Join\n",
    "pd.merge(students, demog, on='student_id', how='outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa94c3ab-4db7-4770-9b6f-af2f75b7f1b6",
   "metadata": {},
   "source": [
    "Outer join keeps all records from both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978c9397-f5b8-4458-89aa-8659fb0c93e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jeft Join\n",
    "pd.merge(students, demog, on='student_id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc921b8-0bcd-4300-9ed2-229cbd027683",
   "metadata": {},
   "source": [
    "A left join retains all records from the left table, displaying missing demographic information as `NaN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a05b880-4c69-450a-8042-66d98f5b6897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Right Join\n",
    "pd.merge(students, demog, on='student_id', how='right')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f981af51-14d3-47a1-9545-b85885f5fa76",
   "metadata": {},
   "source": [
    "A right join keeps all records from the right table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251d87d4-3255-45cf-aa3e-96345026d48d",
   "metadata": {},
   "source": [
    "### Pivoting \"Long\" to \"Wide\"\n",
    "\n",
    "We often store multiple time-varying datasets in a *long* or *stacked* format. For example, in the `scores` dataset, each row represents an individual‚Äôs test score at a specific occasion, rather than storing multiple scores per student in separate columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e346813-0ce6-4420-807a-28078780bf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2953b3-f986-4177-a31f-18ec33fcaed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_wide = scores.pivot(index=\"student_id\", \n",
    "                           columns=\"test_id\",\n",
    "                           values=\"score\")\n",
    "scores_wide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d426815-3eb1-4ba6-a642-ac2409a9b23c",
   "metadata": {},
   "source": [
    "By omitting the last argument, we can create hierarchical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf3edc8-5301-43d3-b66d-49afadc2bfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.pivot(index=\"student_id\", \n",
    "             columns=\"test_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b475724d-52a0-4965-acc3-a762b6d87b4f",
   "metadata": {},
   "source": [
    "### Pivoting \"Wide\" to \"Long\"\n",
    "\n",
    "We can reverse the \"Long\" to \"Wide\" transformation done by `.pivot()` using the `.melt()` function. `.melt()` collapses multiple columns into a single one, resulting in a DataFrame that is $k$ times longer than the input, where $k$ is the number of columns being collapsed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4990b5-22b3-4d0e-9755-f06cce306739",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_wide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cc8332-6d06-4563-b1b8-3a10449abb68",
   "metadata": {},
   "source": [
    "`student_id` is currently set as the index. Before collapsing the columns, we can convert `student_id` back to a column in the DataFrame by using `reset_index()` as shown below.\n",
    "\n",
    "Note that by default, `reset_index()` does NOT modify the DataFrame in place. It returns a new DataFrame with the reset index. If you want to modify the original DataFrame you must assign the result back:\n",
    "\n",
    "```python\n",
    "scores_wide = scores_wide.reset_index()\n",
    "```\n",
    "\n",
    "or use \n",
    "\n",
    "```python\n",
    "scores_wide.reset_index(inplace=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7f371f-adda-4ea9-bf89-66432294595a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_wide.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d7aec5-1d24-4d15-9801-cba855ee7c6c",
   "metadata": {},
   "source": [
    "Now, we can use `melt` with \"student_id\" as the identifier variable and gather all test score variable names into a \"Test\" column, with their corresponding values placed in a \"Score\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3fb2c5-69dd-4507-b5da-546c51b19bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_wide.melt(id_vars=\"student_id\",\n",
    "                 value_vars=[\"Fall\", \"Winter\", \"Spring\"],\n",
    "                 var_name=\"test_id\",\n",
    "                 value_name=\"score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3a9d65-a31c-46b6-b8d8-c4d0f6943982",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Wrap Up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48f3db4-e698-4c1c-a777-67f888aa782b",
   "metadata": {},
   "source": [
    "That's all for now.\n",
    "- Please complete the DC course \"Introduction to Data Visualization with Matplotlib\" by noon on 10/6.\n",
    "- Submit the in-class exercise notebook by 6:00 PM today."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f74d68e-955a-452b-9bd3-22d1b09e4ec1",
   "metadata": {},
   "source": [
    "BY PRINTING YOUR NAME BELOW, YOU CONFIRM THAT THE EXERCISES YOU SUBMITTED IN THIS NOTEBOOK ARE YOUR OWN AND THAT YOU DID NOT USE AI TO ASSIST WITH YOUR WORK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f8f273-7f3e-4ced-a673-dfcafb523798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRINT YOUR NAME\n",
    "print(\"Enter Your Name Here\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
